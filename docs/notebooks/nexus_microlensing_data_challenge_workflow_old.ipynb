{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roman Microlensing Data Challenge 2: Nexus Workflow\n",
    "***\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this tutorial, you, the participant, will be able to:\n",
    "\n",
    "- Load official Data Challenge light curve data from cloud storage.\n",
    "- Install required software in the Roman Research Nexus (RRN).\n",
    "- Initialize a submission project using the `microlens-submit` tool.\n",
    "- Perform a microlensing model fit using `MulensModel`.\n",
    "- Package your model results, plots, and notes into a standardized solution file.\n",
    "- Validate and export your final submission for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook provides the complete, end-to-end workflow for submitting an entry to the Roman Microlensing Data Challenge 2. Following these steps is mandatory for a valid submission.\n",
    "\n",
    "The process involves accessing data directly from the cloud, performing your analysis within the RRN, and using the `microlens-submit` package to standardize your results. This ensures a level playing field and allows the evaluation committee to process all submissions efficiently.\n",
    "\n",
    "### Important Links\n",
    "- **Microlens-Submit Documentation:** [Read the Docs](https://microlens-submit.readthedocs.io/en/latest/)\n",
    "- **RRN Teams & Servers:** For information on setting up a collaborative team server, please see the teams page\n",
    "  <img src=\"../../../images/icons/team.svg\" style=\"vertical-align: bottom; width:1.5em; margin-right:0.5em;\"/> [Working on a Team](../../../markdown/teams.md)\n",
    "- **RRN Software Guide:** [Installing Extra Software](./software.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Installing Required Libraries\n",
    "\n",
    "First, we install the necessary Python packages into our environment. This command reads the `requirements.txt` file you should have placed in this same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "\n",
    "Now we import all the libraries we'll need for this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import microlens_submit\n",
    "import MulensModel\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import emcee\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Access\n",
    "\n",
    "We will now load a light curve directly from the challenge's S3 bucket. You do not need to download anything. We will use `s3fs` to stream the data directly into memory, as shown in the `data_discovery_and_access.md` guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the official, secure path to the data challenge files.\n",
    "TIER = \"2018-test\"\n",
    "EVENT_ID = \"EVENT001\"\n",
    "DATA_URI = f\"s3://roman-dc2-data-public/{TIER}/{EVENT_ID}.dat\"\n",
    "\n",
    "# Establish connection to S3\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Load the data with header and band column\n",
    "print(f\"Loading data from {DATA_URI}...\")\n",
    "with fs.open(DATA_URI, 'r') as f:\n",
    "    data = np.genfromtxt(f, names=True, dtype=None, encoding=None)\n",
    "\n",
    "# Get unique bands\n",
    "bands = np.unique(data['band'])\n",
    "print(f\"Bands found: {bands}\")\n",
    "\n",
    "# Plot each band separately\n",
    "plt.figure(figsize=(8, 5))\n",
    "for band in bands:\n",
    "    mask = data['band'] == band\n",
    "    plt.errorbar(\n",
    "        data['HJD'][mask], data['mag'][mask], yerr=data['mag_err'][mask],\n",
    "        fmt='.', label=f'Band {band}', alpha=0.7\n",
    "    )\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(f\"{EVENT_ID} Light Curve by Band\")\n",
    "plt.xlabel(\"HJD\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.legend(title=\"Band\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Your Submission Project\n",
    "\n",
    "To initialize a submission object in Python, use `microlens_submit.load(project_path)`.\n",
    "If the directory at `project_path` does not exist, it will be created with the correct structure and a blank submission.\n",
    "You can then set the required metadata (like `team_name`, `tier`, etc.) on the returned `Submission` object, and call `.save()` to persist it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your project directory in your persistent home folder\n",
    "project_path = Path(\"./my_dc2_submission\")\n",
    "# To locaate your submission project in the current working directory, use:\n",
    "# project_path = Path.cwd() / \"my_dc2_submission\"\n",
    "# To locate your submission project in a your home directory, use:\n",
    "# project_path = Path.home() / \"my_dc2_submission\"\n",
    "\n",
    "# Now, load the project into our session\n",
    "submission = microlens_submit.load(project_path)  # creates the project directory if it doesn't exist\n",
    "submission.team_name = \"The Transiting Poachers\"\n",
    "submission.tier = TIER\n",
    "print(f\"\\nProject for '{submission.team_name}' loaded successfully.\")\n",
    "\n",
    "# You can expect saving at this point to result in warnings about missing info\n",
    "submission.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing or loading your microlens-sibmit project, and initializing your linked git repository, simply set the repo_url attribute on your Submission. If the directory already exists, load() will just load the existing project, not overwrite it. If the directory exists but no submission content, the content will be added when you run `submission.save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize your git repo\n",
    "!git init\n",
    "!git add .\n",
    "!git commit -m \"Initial commit\"\n",
    "!git branch -m main\n",
    "!git remote add origin https://github.com/yourusername/your-repo.git\n",
    "!git push -u origin main\n",
    "\n",
    "# set the repo url in the submission object\n",
    "submission.repo_url = \"https://github.com/yourusername/your-repo.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You may leave this repository private during the data challenge, but we ask that you make it public once submissions close, to ensure it is accessible to evaluators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to auto-populate your hardware info and are using the Nexus (like the CLI `nexus-init` command does), you can call the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your hardware info\n",
    "submission.autofill_nexus_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will attempt to detect and fill in hardware details if running in the Roman Science Platform environment, but you can always set or override any values manually.\n",
    "\n",
    "```python\n",
    "# add your hardware info\n",
    "submission.hardware_info = {\n",
    "    \"cpu_model\": \"Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz\",\n",
    "    \"num_cores\": 16,\n",
    "    \"memory_gb\": 64,\n",
    "    \"platform\": \"Linux\",\n",
    "    # ...any other relevant info\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A valid submission will include the attributes:\n",
    "* `team_name` to know who the submission belongs to\n",
    "* `tier` to validate event IDs\n",
    "* `repo_url` for reproducability and evaluation\n",
    "* `hardware_info` for benchmarking purposes\n",
    "\n",
    "You can continue to edit your project without all of this information to make it \"valid\", but you will not be able to submit. You can check the submission validity of you entire project at any time using `submission.run_validation()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the project\n",
    "submission.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result in a project directory of the following structure:\n",
    "\n",
    "```\n",
    "   <project_dir>/\n",
    "   â”œâ”€â”€ submission.json\n",
    "   â””â”€â”€ events/\n",
    "```\n",
    "\n",
    "As you add events, solutions, note, and figures they will be populated inside this project as follows:\n",
    "\n",
    "```\n",
    "   <submission_dir>/\n",
    "   â”œâ”€â”€ dossier/\n",
    "   â”œ       â”œâ”€â”€ index.html\n",
    "   â”œ       â”œâ”€â”€ <event_id>.html\n",
    "   â”œ       â”œâ”€â”€ <solution_id>.html \n",
    "   â”œ       â”œâ”€â”€ full_report.html \n",
    "   â”œ       â””â”€â”€ assets/\n",
    "   â”œâ”€â”€ submission.json\n",
    "   â””â”€â”€ events/\n",
    "       â””â”€â”€ <event_id>/\n",
    "           â”œâ”€â”€ event.json\n",
    "           â””â”€â”€ solutions/\n",
    "               â”œâ”€â”€ <lightcurve image>  # your generated lightcurve plots (optional)\n",
    "               â”œâ”€â”€ <lens plane image>  # your generated lens-plane diagram (optional)\n",
    "               â”œâ”€â”€ <posteriors>.csv    # your generated posteriors (optional)\n",
    "               â”œâ”€â”€ <solution_id>.md    # your notes (optional)\n",
    "               â””â”€â”€ <solution_id>.json\n",
    "```\n",
    "\n",
    "> A note for later: We recommend you save generated lightcurve plots, lens-plane diagrams, and notes according to this format, but the submission tool is flexible to all save locations, so long as you include the relative path to your <project_dir> in the <solution_id>.json (accessed through the microlens_submit tool or manually). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Microlensing Model Fitting\n",
    "\n",
    "This is where the science happens. Define your `MulensModel` and use your preferred fitting algorithm (e.g., MCMC) to find the best-fit parameters for the event data. \n",
    "\n",
    "### 5.1. Define the Model\n",
    "As an exmaple, we are goin to do a preliminary fit using only only one band and a point-source-point-lens model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1. Define the model (e.g., Point Source Point Lens)\n",
    "# Use only the first band for this preliminary fit\n",
    "first_band = bands[0]\n",
    "band_mask = data['band'] == first_band\n",
    "band_data = data[band_mask]\n",
    "\n",
    "# Create MulensModel data object for the first band\n",
    "my_data = MulensModel.MulensData(\n",
    "    data_list=[band_data['HJD'].to_numpy(), \n",
    "               band_data['mag'].to_numpy(), \n",
    "               band_data['mag_err'].to_numpy()],\n",
    "    phot_fmt='mag',\n",
    "    bandpass='H'\n",
    ")\n",
    "\n",
    "# Initial model parameters (no parallax for preliminary fit)\n",
    "initial_params = {\n",
    "    't_0': band_data['HJD'].iloc[np.argmin(band_data['mag'])],\n",
    "    'u_0': 0.1,\n",
    "    't_E': 25.,\n",
    "}\n",
    "\n",
    "pspl_model = MulensModel.Model(initial_params)\n",
    "\n",
    "# Create the event object\n",
    "event_object = MulensModel.Event(datasets=my_data, model=pspl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Setting up the Fitting Procedure\n",
    "\n",
    "We will use `emcee` to drive this fit, so we can demonstrate more attributes of the submission object, like the `posterior_path` and `cpu_time` vs `wall_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2. Define the likelihood function for emcee\n",
    "def log_likelihood(theta, event_object, parameters_to_fit):\n",
    "    \"\"\"Log likelihood function for emcee\"\"\"\n",
    "    try:\n",
    "        # Update model parameters\n",
    "        for i, param_name in enumerate(parameters_to_fit):\n",
    "            if param_name == 'log_rho':\n",
    "                # Convert log_rho back to rho\n",
    "                event_object.model.parameters.rho = 10**theta[i]\n",
    "            else:\n",
    "                setattr(event_object.model.parameters, param_name, theta[i])\n",
    "        \n",
    "        # Fit the fluxes given the current model parameters\n",
    "        event_object.fit_fluxes()\n",
    "        \n",
    "        # Get the source and blend fluxes\n",
    "        ([F_S], F_B) = event_object.get_flux_for_dataset(event_object.datasets[0])\n",
    "        \n",
    "        # Calculate chi-squared\n",
    "        chi2 = event_object.get_chi2()\n",
    "        \n",
    "        # Add flux priors if needed (optional)\n",
    "        penalty = 0.0\n",
    "        if F_B <= 0:\n",
    "            penalty = ((F_B / 100)**2)  # Penalize negative blend flux\n",
    "        if F_S <= 0 or (F_S + F_B) <= 0:\n",
    "            return -np.inf  # Return inf if fluxes are non-physical\n",
    "        \n",
    "        return -0.5 * chi2 - penalty\n",
    "    except:\n",
    "        return -np.inf\n",
    "\n",
    "# Set up emcee\n",
    "nwalkers = 32\n",
    "ndim = 3  # t_0, u_0, t_E\n",
    "nsteps = 1000\n",
    "burnin = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Performing the Fit\n",
    "\n",
    "We will use multithreading with `ThreadPool` from the `mutliprocessing` library, for this fit, because it is more compatable with notebooks. However, it caches the state of your notebook and can cause unexpected behaviours when you edit code related to your fit, and re-run. It is a better idea to use a script and multiprocessing using the `Pool` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3. Run MCMC fitting\n",
    "print(\"Starting MCMC fit...\")\n",
    "start_time = time.time()\n",
    "start_cpu = time.process_time()\n",
    "\n",
    "# Set up parameters to fit\n",
    "parameters_to_fit = [\"t_0\", \"u_0\", \"t_E\"]\n",
    "ndim = len(parameters_to_fit)\n",
    "\n",
    "# Initial positions (slightly perturbed from initial guess)\n",
    "pos = np.array([\n",
    "    initial_params['t_0'], initial_params['u_0'], initial_params['t_E']\n",
    "]) + 1e-4 * np.random.randn(nwalkers, ndim)\n",
    "\n",
    "# Use ThreadPool instead of Pool for notebooks\n",
    "n_cores = mp.cpu_count()\n",
    "print(f\"Using {n_cores} threads for MCMC\")\n",
    "\n",
    "# Create the thread pool\n",
    "with ThreadPool(processes=n_cores) as pool:\n",
    "    sampler = emcee.EnsembleSampler(\n",
    "        nwalkers, ndim, log_likelihood, \n",
    "        args=(event_object, parameters_to_fit),\n",
    "        pool=pool\n",
    "    )\n",
    "    sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "\n",
    "# Calculate timing\n",
    "wall_time_hours = (time.time() - start_time) / 3600\n",
    "cpu_time_hours = (time.process_time() - start_cpu) / 3600\n",
    "\n",
    "# Get \"best fit\" parameters (median of posterior)\n",
    "samples = sampler.chain[:, burnin:, :].reshape((-1, ndim))\n",
    "best_fit_params = {\n",
    "    \"t0\": np.median(samples[:, 0]),\n",
    "    \"u0\": np.median(samples[:, 1]),\n",
    "    \"tE\": np.median(samples[:, 2]),\n",
    "}\n",
    "\n",
    "# Update the model with best fit parameters and calculate fluxes\n",
    "for i, param_name in enumerate(parameters_to_fit):\n",
    "    setattr(event_object.model.parameters, param_name, best_fit_params[f\"t{param_name[2:]}\" if param_name.startswith('t_') else param_name.replace('_', '')])\n",
    "\n",
    "# Fit fluxes using MulensModel\n",
    "event_object.fit_fluxes()\n",
    "([F_S], F_B) = event_object.get_flux_for_dataset(event_object.datasets[0])\n",
    "\n",
    "best_fit_params[f\"F{first_band}_S\"] = F_S\n",
    "best_fit_params[f\"F{first_band}_B\"] = F_B\n",
    "\n",
    "# Calculate log likelihood at best fit\n",
    "best_fit_log_likelihood = log_likelihood([\n",
    "    best_fit_params[\"t0\"], \n",
    "    best_fit_params[\"u0\"], \n",
    "    best_fit_params[\"tE\"]\n",
    "], event_object, parameters_to_fit)\n",
    "\n",
    "n_data_points = len(my_data.time)\n",
    "\n",
    "print(f\"Best-fit parameters obtained: {best_fit_params}\")\n",
    "print(f\"Wall time: {wall_time_hours:.3f} hours\")\n",
    "print(f\"CPU time: {cpu_time_hours:.3f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Results\n",
    "Let's save and plot all the fit results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4. Create the initial solution and event objects\n",
    "event = submission.get_event(EVENT_ID)\n",
    "solution = event.add_solution(\n",
    "    model_type=\"1S1L\",\n",
    "    parameters=best_fit_params,\n",
    "    alias=f\"Preliminary PSPL - Band {first_band}\"\n",
    ")\n",
    "\n",
    "# We are doing this now because we want generated solution id for our file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save posteriors\n",
    "posterior_dir = Path(project_path) / \"events\" / EVENT_ID / \"solutions\" / solution.solution_id\n",
    "posterior_dir.mkdir(parents=True, exist_ok=True)\n",
    "posterior_path = posterior_dir / \"posteriors.csv\"\n",
    "\n",
    "# Save posterior samples with column headers (no parallax parameters)\n",
    "posterior_data = np.column_stack([\n",
    "    samples[:, 0], samples[:, 1], samples[:, 2]\n",
    "])\n",
    "np.savetxt(posterior_path, posterior_data, \n",
    "           delimiter=',', \n",
    "           header='t0,u0,tE',\n",
    "           comments='')\n",
    "\n",
    "# Save lightcurve plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(my_data.time, my_data.mag, yerr=my_data.err_mag, fmt='.', \n",
    "             color='k', alpha=0.5, label='Data')\n",
    "\n",
    "# Plot best fit model using the event object\n",
    "event_object.plot_model(color='r', label='Best Fit')\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(f\"{EVENT_ID} Best Fit - Band {first_band}\")\n",
    "plt.xlabel(\"HJD\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "\n",
    "lightcurve_path = posterior_dir / \"lightcurve.png\"\n",
    "plt.savefig(lightcurve_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save caustic diagram (for PSPL, this will be empty since no caustics)\n",
    "plt.figure(figsize=(8, 8))\n",
    "event_object.plot_caustics()\n",
    "plt.title(f\"{EVENT_ID} Caustic Diagram - Band {first_band}\")\n",
    "caustic_path = posterior_dir / \"caustic.png\"\n",
    "plt.savefig(caustic_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Solution Managment\n",
    "\n",
    "Now, we package these results into the submission object.\n",
    "\n",
    "A solution entry has the following attributes:\n",
    "* `model_type`\n",
    "* `parameters` matching the indicated `model_type` and `higher_order_effects`\n",
    "* `higher_order_effects` (if any are present in the model)\n",
    "* `log_likelihood` (the fitâ€™s log-likelihood value; $-\\chi^2/2$)\n",
    "* `relative_probability` (optional, probability of this solution being the true model)\n",
    "* `n_data_points` (number of data points used in the fit)\n",
    "* `compute_info` contanining\n",
    "  - `cpu_hours`\n",
    "  - `wall_time_hours`\n",
    "  - `dependencies` (list of installed Python packages, auto-captured)\n",
    "  - `git_info` (commit, branch, is_dirty; auto-captured if in a git repo)\n",
    "* `t_ref` (reference time, if required for indicated higher-order effects)\n",
    "* `bands` (if the fit is band-dependent)\n",
    "* `alias` (optional, for human-readable, editable solution names)\n",
    "* `notes_path` (path to Markdown notes file, optional but recommended)\n",
    "* `parameter_uncertainties` (uncertainties for parameters)\n",
    "* `physical_parameters` (optional, derived physical parameters)\n",
    "* `posterior_path` (optional, path to posterior samples)\n",
    "* `lightcurve_plot_path` (optional, path to lightcurve plot image)\n",
    "* `lens_plane_plot_path` (optional, path to lens plane plot image)\n",
    "* `used_astrometry` (optional, whether astrometric data was used)\n",
    "* `used_postage_stamps` (optional, whether postage stamp data was used)\n",
    "* `limb_darkening_model` and limb_darkening_coeffs (if relevant)\n",
    "\n",
    "A soltutions requires only these fields for validity:\n",
    "* `model_type`\n",
    "* `parameters` matching the indicated `model_type` with no `higher_order_effects`\n",
    "* `log_likelihood` (the fitâ€™s log-likelihood value; $-\\chi^2/2$)\n",
    "* `n_data_points` (number of data points used in the fit)\n",
    "* `compute_info` contanining at least:\n",
    "  - `cpu_hours`\n",
    "  - `wall_time_hours`\n",
    "\n",
    "You can set these `compute_info` values using the method:\n",
    "`solution.set_compute_info(cpu_hours=..., wall_time_hours=...)`\n",
    "This method also captures:\n",
    "* The current Python environment (pip freeze output)\n",
    "* Git repository info (commit, branch, dirty status)\n",
    "* You can call `set_compute_info()` with either or both values, or leave them blank if you want only the environment info.\n",
    "\n",
    "We calculated these compute times in the previous section using `time.time()` and `time.process_time()`. `time.time()` records the literal time, where as returns the total CPU time (in seconds) that the current process has used since it started. So calculating the diference between to `time.time()` calls will tell you the time elapsed (in seconds) between each call, while the difference between two `time.process_time()` calls will tell you the CPU time elpased, not including time spent sleeping or waiting for I/O. For example, for a process with 4 cores, running for 1 hour, this would mean a CPU time of 4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set compute info\n",
    "# automatically detects the current python environment and git repository state\n",
    "solution.set_compute_info(cpu_hours=cpu_time_hours, wall_time_hours=wall_time_hours)\n",
    "\n",
    "# Set other metadata\n",
    "solution.log_likelihood = log_likelihood\n",
    "solution.n_data_points = n_data_points\n",
    "solution.bands = [str(first_band)]\n",
    "\n",
    "# Set file references\n",
    "solution.posterior_path = str(posterior_path.relative_to(project_path))\n",
    "solution.lightcurve_plot_path = str(lightcurve_path.relative_to(project_path))\n",
    "solution.lens_plane_plot_path = str(caustic_path.relative_to(project_path))\n",
    "\n",
    "# Set notes\n",
    "solution.set_notes(f\"\"\"\n",
    "# Preliminary Fit Analysis\n",
    "\n",
    "This is a preliminary PSPL fit with parallax to determine approximate parameters.\n",
    "\n",
    "## Fit Details\n",
    "- **Model Type**: 1S1L with parallax\n",
    "- **Band Used**: {first_band}\n",
    "- **Data Points**: {n_data_points}\n",
    "- **MCMC Walkers**: {nwalkers}\n",
    "- **MCMC Steps**: {nsteps}\n",
    "- **Burn-in**: {burnin} steps\n",
    "\n",
    "This solution is marked as inactive as it represents a preliminary analysis.\n",
    "\"\"\")\n",
    "\n",
    "# Save everything\n",
    "submission.save()\n",
    "\n",
    "print(f\"âœ… Solution '{solution.alias}' ({solution.solution_id}) created and saved!\")\n",
    "print(f\"   - Posteriors saved to: {posterior_path}\")\n",
    "print(f\"   - Lightcurve plot saved to: {lightcurve_path}\")\n",
    "print(f\"   - Caustic diagram saved to: {caustic_path}\")\n",
    "print(f\"   - Solution marked as inactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dossier and Solution Managment\n",
    "\n",
    "This is the final step. We will generate a human-readable HTML report (a 'dossier'), validate the submission for completeness, and export the final `.zip` file to the official submission location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the HTML Dossier for your own records\n",
    "from microlens_submit.dossier import generate_dashboard_html\n",
    "\n",
    "dossier_path = Path(\"my_project/dossier\")\n",
    "generate_dashboard_html(submission, dossier_path, open=False)\n",
    "print(f\"HTML Dossier report generated at: {dossier_path}/index.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Display the Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dashboard inline (requires IPython)\n",
    "\n",
    "# Read the generated HTML file\n",
    "with open(\"./dossier/index.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Show the dashboard in the notebook\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Display the Event Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the generated HTML file\n",
    "with open(f\"./dossier/{event.event_id}.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Show the dashboard in the notebook\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Display the Solution Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the generated HTML file\n",
    "with open(f\"./dossier/{solution.solution_id}.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Show the dashboard in the notebook\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Dispaly Full Report\n",
    "\n",
    "This is the report that will be generated for the evaluators, with placeholders for sections that are not meant for participants. You can generate this report to view the full state of your project as you go, in the format in which it will be evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the generated HTML file\n",
    "with open(f\"./dossier/full_dossier_report.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Show the dashboard in the notebook\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what if you don't like a fit?\n",
    "\n",
    "You can soft delete a fit without removing it from your local project by deactivating it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deactivate the solution\n",
    "# we are doing this because this is a preliminary fit and we don't want it to \n",
    "# be used in the final report\n",
    "solution.is_active = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also hard delete a solution by calling `event.remove_solution(solution_id, force=True)` or navigating to the solution in the project directory and delete it manually.\n",
    "\n",
    "For these changes to persist outside of the active python objects, you must again save the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation and Submitting the Project\n",
    "\n",
    "This is the final step. We will validate the submission for completeness, and export the final `.zip` file to the official submission location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Validate the submission\n",
    "print(\"\\nRunning validation...\")\n",
    "warnings = submission.run_validation()\n",
    "if warnings:\n",
    "    print(\"âš ï¸ Validation Warnings Found:\")\n",
    "    for w in warnings:\n",
    "        print(f\"  - {w}\")\n",
    "else:\n",
    "    print(\"âœ… Submission is valid and ready for export!\")\n",
    "\n",
    "# 2. Export the final submission file\n",
    "# As the organizer, YOU will replace this placeholder with the real, secure S3 URI.\n",
    "secure_submission_uri = \"s3://roman-dc2-submissions-private/\"\n",
    "output_filename = f\"{submission.team_name.replace(' ', '_')}_{EVENT_ID}.zip\"\n",
    "full_output_path = f\"{secure_submission_uri}{output_filename}\"\n",
    "\n",
    "print(f\"\\nExporting submission to: {full_output_path}\")\n",
    "try:\n",
    "    # Note: Exporting directly to S3 would require s3fs integration\n",
    "    # in microlens-submit, or a temporary local file.\n",
    "    # For now, we export locally and then would manually upload.\n",
    "    local_export_path = project_path / output_filename\n",
    "    submission.export(str(local_export_path))\n",
    "    print(f\"\\nðŸŽ‰ Successfully exported to {local_export_path}\")\n",
    "    print(\"You would now upload this file to the secure submission location.\")\n",
    "except ValueError as e:\n",
    "    print(f\"âŒ Export failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "**Author(s):** Amber Malpas (Data Challenge Organizer) <br>\n",
    "**Keyword(s):** Roman, Microlensing, Data Challenge, Workflow <br>\n",
    "**Last Updated:** July 2025\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
