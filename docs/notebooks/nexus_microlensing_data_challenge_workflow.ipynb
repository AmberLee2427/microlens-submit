{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roman Microlensing Data Challenge 2: Workflow\n",
    "***\n",
    "## Learning Goals\n",
    "\n",
    "By the end of this tutorial, you, the participant, will be able to:\n",
    "\n",
    "- Load official Data Challenge light curve data from cloud storage.\n",
    "- Install required software in the Roman Research Nexus (RRN).\n",
    "- Initialize a submission project using the `microlens-submit` tool.\n",
    "- Perform a microlensing model fit using `MulensModel`.\n",
    "- Package your model results, plots, and notes into a standardized solution file.\n",
    "- Validate and export your final submission for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook provides the complete, end-to-end workflow for submitting an entry to the Roman Microlensing Data Challenge 2. Following these steps is mandatory for a valid submission.\n",
    "\n",
    "The process involves accessing data directly from the cloud, performing your analysis within the RRN, and using the `microlens-submit` package to standardize your results. This ensures a level playing field and allows the evaluation committee to process all submissions efficiently.\n",
    "\n",
    "### Important Links\n",
    "- **Microlens-Submit Documentation:** [Read the Docs](https://microlens-submit.readthedocs.io/en/latest/)\n",
    "- **RRN Teams & Servers:** For information on setting up a collaborative team server, please see the teams page\n",
    "  <img src=\"../../../images/icons/team.svg\" style=\"vertical-align: bottom; width:1.5em; margin-right:0.5em;\"/> [Working on a Team](../../../markdown/teams.md)\n",
    "- **RRN Software Guide:** [Installing Extra Software](./software.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Installing Required Libraries\n",
    "\n",
    "First, we install the necessary Python packages into our environment. This command reads the `requirements.txt` file you should have placed in this same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "\n",
    "Now we import all the libraries we'll need for this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import microlens_submit\n",
    "import MulensModel\n",
    "import s3fs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "import emcee\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Access\n",
    "\n",
    "We will now load a light curve directly from the challenge's S3 bucket. You do not need to download anything. We will use `s3fs` to stream the data directly into memory, as shown in the `data_discovery_and_access.md` guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the official, secure path to the data challenge files.\n",
    "TIER = \"2018-test\"\n",
    "EVENT_ID = \"EVENT001\"\n",
    "DATA_URI = f\"s3://roman-dc2-data-public/{TIER}/{EVENT_ID}.dat\"\n",
    "\n",
    "# Establish connection to S3\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Load the data with header and band column\n",
    "print(f\"Loading data from {DATA_URI}...\")\n",
    "with fs.open(DATA_URI, 'r') as f:\n",
    "    data = np.genfromtxt(f, names=True, dtype=None, encoding=None)\n",
    "\n",
    "# Get unique bands\n",
    "bands = np.unique(data['band'])\n",
    "print(f\"Bands found: {bands}\")\n",
    "\n",
    "# Plot each band separately\n",
    "plt.figure(figsize=(8, 5))\n",
    "for band in bands:\n",
    "    mask = data['band'] == band\n",
    "    plt.errorbar(\n",
    "        data['HJD'][mask], data['mag'][mask], yerr=data['mag_err'][mask],\n",
    "        fmt='.', label=f'Band {band}', alpha=0.7\n",
    "    )\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(f\"{EVENT_ID} Light Curve by Band\")\n",
    "plt.xlabel(\"HJD\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.legend(title=\"Band\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Your Submission Project\n",
    "\n",
    "To initialize a submission object in Python, use `microlens_submit.load(project_path)`.\n",
    "If the directory at `project_path` does not exist, it will be created with the correct structure and a blank submission.\n",
    "You can then set the required metadata (like `team_name`, `tier`, etc.) on the returned `Submission` object, and call `.save()` to persist it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your project directory in your persistent home folder\n",
    "project_path = Path(\"./my_dc2_submission\")\n",
    "# To locate your submission project in the current working directory, use:\n",
    "# project_path = Path.cwd() / \"my_dc2_submission\"\n",
    "# To locate your submission project in your home directory, use:\n",
    "# project_path = Path.home() / \"my_dc2_submission\"\n",
    "\n",
    "# Now, load the project into our session\n",
    "submission = microlens_submit.load(project_path)  # creates the project directory if it doesn't exist\n",
    "submission.team_name = \"The Transiting Poachers\"\n",
    "submission.tier = TIER\n",
    "print(f\"\\nProject for '{submission.team_name}' loaded successfully.\")\n",
    "\n",
    "# You can expect saving at this point to result in warnings about missing info\n",
    "submission.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing or loading your microlens-submit project, and initializing your linked git repository, simply set the repo_url attribute on your Submission. If the directory already exists, load() will just load the existing project, not overwrite it. If the directory exists but no submission content, the content will be added when you run `submission.save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize your git repo\n",
    "!git init\n",
    "!git add .\n",
    "!git commit -m \"Initial commit\"\n",
    "!git branch -m main\n",
    "!git remote add origin https://github.com/yourusername/your-repo.git\n",
    "!git push -u origin main\n",
    "\n",
    "# set the repo url in the submission object\n",
    "submission.repo_url = \"https://github.com/yourusername/your-repo.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You may leave this repository private during the data challenge, but we ask that you make it public once submissions close, to ensure it is accessible to evaluators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to auto-populate your hardware info and are using the Nexus (like the CLI `nexus-init` command does), you can call the method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your hardware info\n",
    "submission.autofill_nexus_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will attempt to detect and fill in hardware details if running in the Roman Science Platform environment, but you can always set or override any values manually.\n",
    "\n",
    "```python\n",
    "# add your hardware info\n",
    "submission.hardware_info = {\n",
    "    \"cpu_model\": \"Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz\",\n",
    "    \"num_cores\": 16,\n",
    "    \"memory_gb\": 64,\n",
    "    \"platform\": \"Linux\",\n",
    "    # ...any other relevant info\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A valid submission will include the attributes:\n",
    "* `team_name` to know who the submission belongs to\n",
    "* `tier` to validate event IDs\n",
    "* `repo_url` for reproducibility and evaluation\n",
    "* `hardware_info` for benchmarking purposes\n",
    "\n",
    "You can continue to edit your project without all of this information to make it \"valid\", but you will not be able to submit. You can check the submission validity of your entire project at any time using `submission.run_validation()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Command Line Interface (CLI)\n",
    "\n",
    "If you prefer using the command line or want to automate parts of your workflow, you can accomplish the same tasks using the `microlens-submit` CLI. The CLI is particularly useful for batch processing and automation.\n",
    "\n",
    "### Nexus-Specific CLI Commands\n",
    "\n",
    "The following commands are specifically designed for the Roman Research Nexus environment:\n",
    "\n",
    "#### 1. Initialize Project with Nexus Hardware Info\n",
    "\n",
    "The `nexus-init` command automatically detects and records your Nexus environment details:\n",
    "\n",
    "```bash\n",
    "# Initialize project with automatic Nexus hardware detection\n",
    "microlens-submit nexus-init --team-name \"Your Team\" --tier \"2018-test\" ./my_dc2_submission\n",
    "\n",
    "# This command will automatically detect:\n",
    "# - CPU model from /proc/cpuinfo\n",
    "# - Memory from /proc/meminfo\n",
    "# - Nexus image from JUPYTER_IMAGE_SPEC\n",
    "# - Platform information\n",
    "```\n",
    "\n",
    "#### 2. Set Hardware Information Manually\n",
    "\n",
    "You can also set hardware information manually using the `set-hardware-info` command:\n",
    "\n",
    "```bash\n",
    "# Set CPU information\n",
    "microlens-submit set-hardware-info --cpu \"Intel(R) Xeon(R) CPU E5-2670 v3 @ 2.30GHz\" \\\n",
    "    --memory-gb 64 --platform \"Roman Research Nexus\" \\\n",
    "    --nexus-image \"roman-dc2:latest\"\n",
    "\n",
    "# Or set individual components\n",
    "microlens-submit set-hardware-info --cpu-details \"16-core Intel Xeon\" \\\n",
    "    --ram-gb 64 --platform \"Roman Science Platform\"\n",
    "```\n",
    "\n",
    "### Complete CLI Workflow Example\n",
    "\n",
    "Here's how to accomplish the same tasks using the command-line interface:\n",
    "\n",
    "```bash\n",
    "# 1. Initialize project with Nexus hardware info\n",
    "microlens-submit nexus-init --team-name \"The Transiting Poachers\" --tier \"2018-test\" ./my_dc2_submission\n",
    "\n",
    "# 2. Add a solution with parameters\n",
    "microlens-submit add-solution EVENT001 1S1L \\\n",
    "    --param t0=2459123.5 --param u0=0.15 --param tE=20.5 \\\n",
    "    --log-likelihood -1234.56 --n-data-points 1250 \\\n",
    "    --cpu-hours 15.2 --wall-time-hours 3.8 \\\n",
    "    --lightcurve-plot-path plots/event001_lc.png \\\n",
    "    --lens-plane-plot-path plots/event001_lens.png \\\n",
    "    --notes \"Initial PSPL fit using MulensModel and emcee\"\n",
    "\n",
    "# 3. Add a more complex solution\n",
    "microlens-submit add-solution EVENT001 1S2L \\\n",
    "    --param t0=2459123.5 --param u0=0.12 --param tE=22.1 \\\n",
    "    --param q=0.001 --param s=1.15 --param alpha=45.2 \\\n",
    "    --log-likelihood -1189.34 --n-data-points 1250 \\\n",
    "    --cpu-hours 28.5 --wall-time-hours 7.2 \\\n",
    "    --alias \"planetary_fit\" \\\n",
    "    --notes \"Binary lens fit with planetary companion\"\n",
    "\n",
    "# 4. Compare solutions\n",
    "microlens-submit compare-solutions EVENT001\n",
    "\n",
    "# 5. Validate your submission\n",
    "microlens-submit validate-submission\n",
    "\n",
    "# 6. Generate dossier for review\n",
    "microlens-submit generate-dossier\n",
    "\n",
    "# 7. Export final submission\n",
    "microlens-submit export final_submission.zip\n",
    "```\n",
    "\n",
    "### CLI vs Python API\n",
    "\n",
    "**When to use CLI:**\n",
    "- Batch processing multiple events\n",
    "- Automation and scripting\n",
    "- Quick parameter updates\n",
    "- Command-line workflows\n",
    "- Integration with other tools\n",
    "\n",
    "**When to use Python API:**\n",
    "- Interactive analysis in Jupyter notebooks\n",
    "- Complex data processing\n",
    "- Custom validation logic\n",
    "- Integration with scientific Python ecosystem\n",
    "\n",
    "### Additional CLI Resources\n",
    "\n",
    "For a comprehensive CLI tutorial, see the [Command Line Tutorial](https://microlens-submit.readthedocs.io/en/latest/tutorial.html) in the documentation.\n",
    "\n",
    "For detailed API reference, see the [API Documentation](https://microlens-submit.readthedocs.io/en/latest/api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the project\n",
    "submission.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result in a project directory of the following structure:\n",
    "\n",
    "```\n",
    "   <project_dir>/\n",
    "   ├── submission.json\n",
    "   └── events/\n",
    "```\n",
    "\n",
    "As you add events, solutions, notes, and figures they will be populated inside this project as follows:\n",
    "\n",
    "```\n",
    "   <submission_dir>/\n",
    "   ├── dossier/\n",
    "   ├       ├── index.html\n",
    "   ├       ├── <event_id>.html\n",
    "   ├       ├── <solution_id>.html \n",
    "   ├       ├── full_report.html \n",
    "   ├       └── assets/\n",
    "   ├── submission.json\n",
    "   └── events/\n",
    "       └── <event_id>/\n",
    "           ├── event.json\n",
    "           └── solutions/\n",
    "               ├── <lightcurve image>  # your generated lightcurve plots (optional)\n",
    "               ├── <lens plane image>  # your generated lens-plane diagram (optional)\n",
    "               ├── <posteriors>.csv    # your generated posteriors (optional)\n",
    "               ├── <solution_id>.md    # your notes (optional)\n",
    "               └── <solution_id>.json\n",
    "```\n",
    "\n",
    "> A note for later: We recommend you save generated lightcurve plots, lens-plane diagrams, and notes according to this format, but the submission tool is flexible to all save locations, so long as you include the relative path to your <project_dir> in the <solution_id>.json (accessed through the microlens_submit tool or manually). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Microlensing Model Fitting\n",
    "\n",
    "This is where the science happens. Define your `MulensModel` and use your preferred fitting algorithm (e.g., MCMC) to find the best-fit parameters for the event data. \n",
    "\n",
    "### 5.1. Define the Model\n",
    "As an example, we are going to do a preliminary fit using only one band and a point-source-point-lens model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1. Define the model (e.g., Point Source Point Lens)\n",
    "# Use only the first band for this preliminary fit\n",
    "first_band = bands[0]\n",
    "band_mask = data['band'] == first_band\n",
    "band_data = data[band_mask]\n",
    "\n",
    "# Create MulensModel data object for the first band\n",
    "my_data = MulensModel.MulensData(\n",
    "    data_list=[band_data['HJD'], \n",
    "               band_data['mag'], \n",
    "               band_data['mag_err']],\n",
    "    phot_fmt='mag',\n",
    "    bandpass='H'\n",
    ")\n",
    "\n",
    "# Initial model parameters (no parallax for preliminary fit)\n",
    "initial_params = {\n",
    "    't_0': band_data['HJD'][np.argmin(band_data['mag'])],\n",
    "    'u_0': 0.1,\n",
    "    't_E': 25.,\n",
    "}\n",
    "\n",
    "pspl_model = MulensModel.Model(initial_params)\n",
    "\n",
    "# Create the event object\n",
    "event_object = MulensModel.Event(datasets=my_data, model=pspl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Setting up the Fitting Procedure\n",
    "\n",
    "We will use `emcee` to drive this fit, so we can demonstrate more attributes of the submission object, like the `posterior_path` and `cpu_time` vs `wall_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2. Define the likelihood function for emcee\n",
    "def log_likelihood(theta, event_object, parameters_to_fit):\n",
    "    \"\"\"Log likelihood function for emcee\"\"\"\n",
    "    try:\n",
    "        # Update model parameters\n",
    "        for i, param_name in enumerate(parameters_to_fit):\n",
    "            if param_name == 'log_rho':\n",
    "                # Convert log_rho back to rho\n",
    "                event_object.model.parameters.rho = 10**theta[i]\n",
    "            else:\n",
    "                setattr(event_object.model.parameters, param_name, theta[i])\n",
    "        \n",
    "        # Fit the fluxes given the current model parameters\n",
    "        event_object.fit_fluxes()\n",
    "        \n",
    "        # Get the source and blend fluxes\n",
    "        ([F_S], F_B) = event_object.get_flux_for_dataset(event_object.datasets[0])\n",
    "        \n",
    "        # Calculate chi-squared\n",
    "        chi2 = event_object.get_chi2()\n",
    "        \n",
    "        # Add flux priors if needed (optional)\n",
    "        penalty = 0.0\n",
    "        if F_B <= 0:\n",
    "            penalty = ((F_B / 100)**2)  # Penalize negative blend flux\n",
    "        if F_S <= 0 or (F_S + F_B) <= 0:\n",
    "            return -np.inf  # Return inf if fluxes are non-physical\n",
    "        \n",
    "        return -0.5 * chi2 - penalty\n",
    "    except:\n",
    "        return -np.inf\n",
    "\n",
    "# Set up emcee\n",
    "nwalkers = 32\n",
    "ndim = 3  # t_0, u_0, t_E\n",
    "nsteps = 1000\n",
    "burnin = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Performing the Fit\n",
    "\n",
    "We will use multithreading with `ThreadPool` from the `multiprocessing` library for this fit, because it is more compatible with notebooks. However, it caches the state of your notebook and can cause unexpected behaviors when you edit code related to your fit and re-run. It is a better idea to use a script and multiprocessing using the `Pool` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3. Run MCMC fitting\n",
    "print(\"Starting MCMC fit...\")\n",
    "start_time = time.time()\n",
    "start_cpu = time.process_time()\n",
    "\n",
    "# Set up parameters to fit\n",
    "parameters_to_fit = [\"t_0\", \"u_0\", \"t_E\"]\n",
    "ndim = len(parameters_to_fit)\n",
    "\n",
    "# Initial positions (slightly perturbed from initial guess)\n",
    "pos = np.array([\n",
    "    initial_params['t_0'], initial_params['u_0'], initial_params['t_E']\n",
    "]) + 1e-4 * np.random.randn(nwalkers, ndim)\n",
    "\n",
    "# Use ThreadPool instead of Pool for notebooks\n",
    "n_cores = mp.cpu_count()\n",
    "print(f\"Using {n_cores} threads for MCMC\")\n",
    "\n",
    "# Create the thread pool\n",
    "with ThreadPool(processes=n_cores) as pool:\n",
    "    sampler = emcee.EnsembleSampler(\n",
    "        nwalkers, ndim, log_likelihood, \n",
    "        args=(event_object, parameters_to_fit),\n",
    "        pool=pool\n",
    "    )\n",
    "    sampler.run_mcmc(pos, nsteps, progress=True)\n",
    "\n",
    "# Calculate timing\n",
    "wall_time_hours = (time.time() - start_time) / 3600\n",
    "cpu_time_hours = (time.process_time() - start_cpu) / 3600\n",
    "\n",
    "# Get \"best fit\" parameters (median of posterior)\n",
    "samples = sampler.chain[:, burnin:, :].reshape((-1, ndim))\n",
    "best_fit_params = {\n",
    "    \"t0\": np.median(samples[:, 0]),\n",
    "    \"u0\": np.median(samples[:, 1]),\n",
    "    \"tE\": np.median(samples[:, 2]),\n",
    "}\n",
    "\n",
    "# Update the model with best fit parameters and calculate fluxes\n",
    "for i, param_name in enumerate(parameters_to_fit):\n",
    "    setattr(event_object.model.parameters, param_name, best_fit_params[f\"t{param_name[2:]}\" if param_name.startswith('t_') else param_name.replace('_', '')])\n",
    "\n",
    "# Fit fluxes using MulensModel\n",
    "event_object.fit_fluxes()\n",
    "([F_S], F_B) = event_object.get_flux_for_dataset(event_object.datasets[0])\n",
    "\n",
    "best_fit_params[f\"F{first_band}_S\"] = F_S\n",
    "best_fit_params[f\"F{first_band}_B\"] = F_B\n",
    "\n",
    "# Calculate log likelihood at best fit\n",
    "best_log_likelihood = log_likelihood([\n",
    "    best_fit_params[\"t0\"], \n",
    "    best_fit_params[\"u0\"], \n",
    "    best_fit_params[\"tE\"]\n",
    "], event_object, parameters_to_fit)\n",
    "\n",
    "n_data_points = len(my_data.time)\n",
    "\n",
    "print(f\"Best-fit parameters obtained: {best_fit_params}\")\n",
    "print(f\"Wall time: {wall_time_hours:.3f} hours\")\n",
    "print(f\"CPU time: {cpu_time_hours:.3f} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Results\n",
    "Let's save and plot all the fit results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4. Create the initial solution and event objects\n",
    "event = submission.get_event(EVENT_ID)\n",
    "solution = event.add_solution(\n",
    "    model_type=\"1S1L\",\n",
    "    parameters=best_fit_params,\n",
    "    alias=f\"Preliminary PSPL - Band {first_band}\"\n",
    ")\n",
    "\n",
    "# We are doing this now because we want generated solution id for our file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save posteriors\n",
    "posterior_dir = Path(project_path) / \"events\" / EVENT_ID / \"solutions\" / solution.solution_id\n",
    "posterior_dir.mkdir(parents=True, exist_ok=True)\n",
    "posterior_path = posterior_dir / \"posteriors.csv\"\n",
    "\n",
    "# Save posterior samples with column headers (no parallax parameters)\n",
    "posterior_data = np.column_stack([\n",
    "    samples[:, 0], samples[:, 1], samples[:, 2]\n",
    "])\n",
    "np.savetxt(posterior_path, posterior_data, \n",
    "           delimiter=',', \n",
    "           header='t0,u0,tE',\n",
    "           comments='')\n",
    "\n",
    "# Save lightcurve plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(my_data.time, my_data.mag, yerr=my_data.err_mag, fmt='.', \n",
    "             color='k', alpha=0.5, label='Data')\n",
    "\n",
    "# Plot best fit model using the event object\n",
    "event_object.plot_model(color='r', label='Best Fit')\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(f\"{EVENT_ID} Best Fit - Band {first_band}\")\n",
    "plt.xlabel(\"HJD\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "\n",
    "lightcurve_path = posterior_dir / \"lightcurve.png\"\n",
    "plt.savefig(lightcurve_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save caustic diagram (for PSPL, this will be empty since no caustics)\n",
    "plt.figure(figsize=(8, 8))\n",
    "event_object.plot_caustics()\n",
    "plt.title(f\"{EVENT_ID} Caustic Diagram - Band {first_band}\")\n",
    "caustic_path = posterior_dir / \"caustic.png\"\n",
    "plt.savefig(caustic_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Solution Management\n",
    "\n",
    "Now, we package these results into the submission object.\n",
    "\n",
    "A solution entry has the following attributes:\n",
    "* `model_type`\n",
    "* `parameters` matching the indicated `model_type` and `higher_order_effects`\n",
    "* `higher_order_effects` (if any are present in the model)\n",
    "* `log_likelihood` (the fit's log-likelihood value; $-\\chi^2/2$)\n",
    "* `relative_probability` (optional, probability of this solution being the true model)\n",
    "* `n_data_points` (number of data points used in the fit)\n",
    "* `compute_info` containing:\n",
    "  - `cpu_hours`\n",
    "  - `wall_time_hours`\n",
    "  - `dependencies` (list of installed Python packages, auto-captured)\n",
    "  - `git_info` (commit, branch, is_dirty; auto-captured if in a git repo)\n",
    "* `t_ref` (reference time, if required for indicated higher-order effects)\n",
    "* `bands` (if the fit is band-dependent)\n",
    "* `alias` (optional, for human-readable, editable solution names)\n",
    "* `notes_path` (path to Markdown notes file, optional but recommended)\n",
    "* `parameter_uncertainties` (uncertainties for parameters)\n",
    "* `physical_parameters` (optional, derived physical parameters)\n",
    "* `posterior_path` (optional, path to posterior samples)\n",
    "* `lightcurve_plot_path` (optional, path to lightcurve plot image)\n",
    "* `lens_plane_plot_path` (optional, path to lens plane plot image)\n",
    "* `used_astrometry` (optional, whether astrometric data was used)\n",
    "* `used_postage_stamps` (optional, whether postage stamp data was used)\n",
    "* `limb_darkening_model` and limb_darkening_coeffs (if relevant)\n",
    "\n",
    "A solution requires only these fields for validity:\n",
    "* `model_type`\n",
    "* `parameters` matching the indicated `model_type` with no `higher_order_effects`\n",
    "* `log_likelihood` (the fit's log-likelihood value; $-\\chi^2/2$)\n",
    "* `n_data_points` (number of data points used in the fit)\n",
    "* `compute_info` containing at least:\n",
    "  - `cpu_hours`\n",
    "  - `wall_time_hours`\n",
    "\n",
    "You can set these `compute_info` values using the method:\n",
    "`solution.set_compute_info(cpu_hours=..., wall_time_hours=...)`\n",
    "This method also captures:\n",
    "* The current Python environment (pip freeze output)\n",
    "* Git repository info (commit, branch, dirty status)\n",
    "* You can call `set_compute_info()` with either or both values, or leave them blank if you want only the environment info.\n",
    "\n",
    "We calculated these compute times in the previous section using `time.time()` and `time.process_time()`. `time.time()` records the literal time, whereas `time.process_time()` returns the total CPU time (in seconds) that the current process has used since it started. So calculating the difference between two `time.time()` calls will tell you the time elapsed (in seconds) between each call, while the difference between two `time.process_time()` calls will tell you the CPU time elapsed, not including time spent sleeping or waiting for I/O. For example, for a process with 4 cores, running for 1 hour, this would mean a CPU time of 4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set compute info\n",
    "# automatically detects the current python environment and git repository state\n",
    "solution.set_compute_info(cpu_hours=cpu_time_hours, wall_time_hours=wall_time_hours)\n",
    "\n",
    "# Set other metadata\n",
    "solution.log_likelihood = best_log_likelihood\n",
    "solution.n_data_points = n_data_points\n",
    "solution.bands = [str(first_band)]\n",
    "\n",
    "# Set file references\n",
    "solution.posterior_path = str(posterior_path.relative_to(project_path))\n",
    "solution.lightcurve_plot_path = str(lightcurve_path.relative_to(project_path))\n",
    "solution.lens_plane_plot_path = str(caustic_path.relative_to(project_path))\n",
    "\n",
    "# Set notes\n",
    "solution.set_notes(f\"\"\"\n",
    "# Preliminary Fit Analysis\n",
    "\n",
    "This is a preliminary PSPL fit to determine approximate parameters.\n",
    "\n",
    "## Fit Details\n",
    "- **Model Type**: 1S1L\n",
    "- **Band Used**: {first_band}\n",
    "- **Data Points**: {n_data_points}\n",
    "- **MCMC Walkers**: {nwalkers}\n",
    "- **MCMC Steps**: {nsteps}\n",
    "- **Burn-in**: {burnin} steps\n",
    "\n",
    "This solution is marked as inactive as it represents a preliminary analysis.\n",
    "\"\"\")\n",
    "\n",
    "# Save everything\n",
    "submission.save()\n",
    "\n",
    "print(f\"✅ Solution '{solution.alias}' ({solution.solution_id}) created and saved!\")\n",
    "print(f\"   - Posteriors saved to: {posterior_path}\")\n",
    "print(f\"   - Lightcurve plot saved to: {lightcurve_path}\")\n",
    "print(f\"   - Caustic diagram saved to: {caustic_path}\")\n",
    "print(f\"   - Solution marked as inactive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Solution Comparison and Relative Probability\n",
    "\n",
    "When you have multiple solutions for the same event, you can specify their relative probabilities to indicate which model you think is most likely to be correct.\n",
    "\n",
    "**Key Points:**\n",
    "* **Relative probabilities must sum to 1.0** across all active solutions for an event\n",
    "* **If you don't specify relative probabilities**, they are automatically calculated during export using the Bayesian Information Criterion (BIC)\n",
    "* **BIC calculation** uses: `BIC = k*ln(n) - 2*log_likelihood` where k = number of parameters, n = number of data points\n",
    "* **Equal probabilities** are assigned if BIC calculation is not possible (missing data)\n",
    "\n",
    "**Example of manual relative probability assignment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: If you have multiple solutions for the same event\n",
    "# solution1.relative_probability = 0.7  # 70% confidence\n",
    "# solution2.relative_probability = 0.3  # 30% confidence\n",
    "# Total must equal 1.0\n",
    "\n",
    "# For this example, we'll just note that relative probability is optional\n",
    "# and will be calculated automatically if not provided\n",
    "print(\"Relative probability will be calculated automatically during export if not specified.\")\n",
    "print(\"The calculation uses BIC: BIC = k*ln(n) - 2*log_likelihood\")\n",
    "print(\"where k = number of parameters, n = number of data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How automatic calculation works during export:**\n",
    "1. **If you provide relative probabilities** that sum to 1.0, those are used\n",
    "2. **If some solutions have relative probabilities** but others don't, the remaining probability is distributed among the unspecified solutions using BIC\n",
    "3. **If no solutions have relative probabilities**, all active solutions get equal probability\n",
    "4. **If BIC calculation fails** (missing log_likelihood, n_data_points, or parameters), equal probabilities are assigned\n",
    "\n",
    "This ensures that evaluators always have a complete probability distribution for your solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dossier and Solution Management\n",
    "\n",
    "This is the final step. We will generate a human-readable HTML report (a 'dossier'), validate the submission for completeness, and export the final `.zip` file to the official submission location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the HTML Dossier for your own records\n",
    "from microlens_submit.dossier import generate_dashboard_html\n",
    "\n",
    "dossier_path = project_path / \"dossier\"\n",
    "generate_dashboard_html(submission, dossier_path, open=False)\n",
    "print(f\"HTML Dossier report generated at: {dossier_path}/index.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Display the Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dashboard inline (requires IPython)\n",
    "\n",
    "# Read the generated HTML file\n",
    "with open(dossier_path / \"index.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Show the dashboard in the notebook\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Display the Event Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the generated HTML file\n",
    "with open(dossier_path / f\"{event.event_id}.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Show the dashboard in the notebook\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Display the Solution Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the generated HTML file\n",
    "with open(dossier_path / f\"{solution.solution_id}.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Show the dashboard in the notebook\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Display Full Report\n",
    "\n",
    "This is the report that will be generated for the evaluators, with placeholders for sections that are not meant for participants. You can generate this report to view the full state of your project as you go, in the format in which it will be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the generated HTML file\n",
    "with open(dossier_path / \"full_dossier_report.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "# Show the dashboard in the notebook\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what if you don't like a fit?\n",
    "\n",
    "You can soft delete a fit without removing it from your local project by deactivating it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deactivate the solution\n",
    "# we are doing this because this is a preliminary fit and we don't want it to \n",
    "# be used in the final report\n",
    "solution.is_active = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also hard delete a solution by calling `event.remove_solution(solution_id, force=True)` or navigating to the solution in the project directory and delete it manually.\n",
    "\n",
    "For these changes to persist outside of the active python objects, you must again save the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation and Submitting the Project\n",
    "\n",
    "This is the final step. We will validate the submission for completeness, and export the final `.zip` file to the official submission location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Validate the submission\n",
    "print(\"\\nRunning validation...\")\n",
    "warnings = submission.run_validation()\n",
    "if warnings:\n",
    "    print(\"⚠️ Validation Warnings Found:\")\n",
    "    for w in warnings:\n",
    "        print(f\"  - {w}\")\n",
    "else:\n",
    "    print(\"✅ Submission is valid and ready for export!\")\n",
    "\n",
    "# 2. Export the final submission file\n",
    "# As the organizer, YOU will replace this placeholder with the real, secure S3 URI.\n",
    "secure_submission_uri = \"s3://roman-dc2-submissions-private/\"\n",
    "output_filename = f\"{submission.team_name.replace(' ', '_')}_{EVENT_ID}.zip\"\n",
    "full_output_path = f\"{secure_submission_uri}{output_filename}\"\n",
    "\n",
    "print(f\"\\nExporting submission to: {full_output_path}\")\n",
    "try:\n",
    "    # Note: Exporting directly to S3 would require s3fs integration\n",
    "    # in microlens-submit, or a temporary local file.\n",
    "    # For now, we export locally and then would manually upload.\n",
    "    local_export_path = project_path / output_filename\n",
    "    submission.export(str(local_export_path))\n",
    "    print(f\"\\n🎉 Successfully exported to {local_export_path}\")\n",
    "    print(\"You would now upload this file to the secure submission location.\")\n",
    "except ValueError as e:\n",
    "    print(f\"❌ Export failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "**Author(s):** Amber Malpas (Data Challenge Organizer) <br>\n",
    "**Keyword(s):** Roman, Microlensing, Data Challenge, Workflow <br>\n",
    "**Last Updated:** July 2025\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
