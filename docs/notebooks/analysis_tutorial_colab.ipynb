{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVT-IDqXWVdf"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <a href=\"https://github.com/AmberLee2427/microlens-submit\">\n",
        "    <img src=\"../_static/rges-pit_logo.png\" alt=\"logo\" width=\"300\"/>\n",
        "  </a>\n",
        "</div>\n",
        "\n",
        "# <font face=\"Helvetica\" size=\"7\"> Microlensing Analysis Notebook </font>  \n",
        "\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/AmberLee2427/microlens-submit/blob/main/docs/notebooks/analysis_tutorial_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> <i> Authors: Amber Malpas, Katarzyna Kruszyńska, Ali Crisp </i>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "If you would like an introduction to python notebooks, please read this tutorial: https://medium.com/codingthesmartway-com-blog/getting-started-with-jupyter-notebook-for-python-4e7082bd5d46\n",
        "\n",
        "## <font face=\"Helvetica\" size=\"6\"> Installation and Set-Up </font>\n",
        "\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "Please note, you must **save this notebook in a space owned by you** (a GitHub repo, gist, to Google Drive, or locally) if you want to come back to it later without losing your progress. You can edit and run this notebook on Colab, but it **will not auto save** for you.\n",
        "\n",
        "If you choose to use lcoal resources your notebook will use your local packages, so you should follow install a virtual environment with the following packages. Run the cell below to create a downloadable `.yml` file, to automate the package install process (provided you are using anaconda)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNyjmctOWfqE"
      },
      "outputs": [],
      "source": [
        "yaml = '''name: roman_fit\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.11\n",
        "  - numpy\n",
        "  - matplotlib\n",
        "  - pandas\n",
        "  - scipy\n",
        "  - jupyter\n",
        "  - ipython\n",
        "  - astropy\n",
        "  - beautifulsoup4\n",
        "  - lxml              # required parser for bs4\n",
        "  - pip\n",
        "  - pip:\n",
        "      - pathos\n",
        "      - MulensModel'''\n",
        "\n",
        "# save the yaml\n",
        "with open('environment.yml', 'w') as f:\n",
        "    f.write(yaml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coWgvLD5Waaq"
      },
      "source": [
        "Click the folder button on the side bar to open the file explorer. The file `environment.yml` should be in there now. Just click the triple dots on the side and then `Download` to download the `.yml` file.\n",
        "\n",
        "```bash\n",
        "conda env create -f environment.yml\n",
        "```\n",
        "\n",
        "Running the above line in a terminal (Anaconda Prompt on Windows) will create a virtual conda environment called `minicourse`, which has the required packages installed.\n",
        "\n",
        "You can activate the environment with:\n",
        "\n",
        "```bash\n",
        "conda activate minicourse\n",
        "```\n",
        "\n",
        "From here you have two options\n",
        "\n",
        "1. You can open the notebook running\n",
        "```bash\n",
        "jupyter notebook\n",
        "```\n",
        "from a parent folder to your locally saved version of this notebook and navigating to the notebook in your browser. You may need to select `minicourse` as your kernel before running the notebook.\n",
        "\n",
        "2. Alternatly, you can create a local \"Runtime\" and for your Colab notebook by following [these instructions](https://www.google.com/url?q=https%3A%2F%2Fresearch.google.com%2Fcolaboratory%2Flocal-runtimes.html).\n",
        "```bash\n",
        "jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --no-browser\n",
        "```\n",
        "\n",
        "  ⚠️ We recommend you take care when doing this with notebooks that you didn't write as it gives them access to your local machine.\n",
        "\n",
        "Before continuing with this notebook, **please run the following import and set-up cells** by pressing the play button or `SHFT + ENTR`.\n",
        "\n",
        "<!--\n",
        "## <font face=\"Helvetica\" size=\"6\"> Dev Notes </font>\n",
        "<hr style=\"border: 1.5pt solid #fc3d21; width: 100%; margin-top: -10px;\">\n",
        "-->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PDvU-MTkyi0K",
        "outputId": "a43df760-1f80-4415-dd10-95a95e9f205b"
      },
      "outputs": [],
      "source": [
        "#@title General Imports\n",
        "\n",
        "# system tools\n",
        "import os\n",
        "import sys\n",
        "from io import StringIO\n",
        "import time\n",
        "from typing import Tuple, Callable, Optional, List\n",
        "import shutil\n",
        "\n",
        "# data analysis tools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from scipy.optimize import minimize\n",
        "import astropy.units as u\n",
        "from astropy.coordinates import Angle, SkyCoord\n",
        "try:\n",
        "    from google.colab import sheets  # will only work if you are running on Colab\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# web scrapping tools\n",
        "import bs4 as bs\n",
        "import urllib\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "\n",
        "# parallel processing tools\n",
        "!pip install pathos\n",
        "from pathos.multiprocessing import ProcessingPool as Pool  # for multiprocessing inside jupyter\n",
        "import multiprocessing as mp  # Ensure this is imported"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNIm2yQgMmoK",
        "outputId": "f25c8e85-9828-4c43-998f-25b04ade5b29"
      },
      "outputs": [],
      "source": [
        "#@title Store and reset the notebooks working directory\n",
        "# This cell restores the working directory in case of failure.\n",
        "# Make sure you run this cell before running the MulensModel package fix\n",
        "\n",
        "def reset_cwd():\n",
        "  # Declare _cwd as global within the function's scope\n",
        "  global _cwd\n",
        "\n",
        "  cwd = os.getcwd()\n",
        "  print('current working directory:',cwd)\n",
        "  try:\n",
        "      # Use the global _cwd\n",
        "      os.chdir(_cwd)\n",
        "  except NameError:\n",
        "      # _cwd is not yet defined, so initialize it\n",
        "      _cwd = cwd  # saves the stored _cwd to the wd when the cell/function is first run\n",
        "      os.chdir(_cwd) # change to the newly initialized directory\n",
        "\n",
        "  print('working directory reset to:', _cwd)\n",
        "\n",
        "  return _cwd\n",
        "\n",
        "cwd = reset_cwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TA6gvhQWmaV"
      },
      "source": [
        "## <font face=\"Helvetica\" size=\"6\"> Introduction </font>\n",
        "\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "Welcome to the **Data Challenge: Assistance Notebook**.\n",
        "\n",
        "This notebook is brought to you by the RGES-PIT and is inteneded to be an instroductory workbook for users new to microlensing event fitting, users who would like a refresher, or users who would like a comprehensive introduction to tools they have not used before.\n",
        "\n",
        "The data challenge is intended to be a semi-realistic representation of the data volume and type expected from the Roman Galactic Bulge Time Domain Survey. Specifically for microlensing events and microlensing false positives.  \n",
        "\n",
        "Our aim is to provide you with a realistic view of what working with bulk microlensing data involves. This tool is designed to help you build confidence in managing large datasets and using etablished fitting tools.\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> What data are used in this notebook? </font>\n",
        "\n",
        "This notebook primarily uses lightcurves from [Data Challenge 1](https://www.microlensing-source.org/data-challenge/). None of the fits inside this notebook are in anyway performing the tasks of Data Challenge 2. The data will be [cloned](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) and sorted in later sections. Because Data Challenge 1 concluded in 2018, we can make full use of the [parameter \"truths\"](https://en.wikipedia.org/wiki/Statistical_parameter#:~:text=parameter%20describes%20the-,true%20value,-calculated%20from%20the) for each event to verify our fitting processes.\n",
        "\n",
        "The dataset consists of two lightcurve files for each event or star, representing the data from Roman's `W149` and `Z087` filters. The files are in ASCII format with the columns BJD, Aperture_Magnitude and Error, and follow the file-naming convention: `ulwdc1_nnn_[W149/Z087].txt`\n",
        "\n",
        "Supplementary files were also provided including `wfirst_ephemeris.txt`, which contains the `BJD` and 3D spacecraft location within the solar system. Information was provided on the surface-brightness color relation for `Z087-W149` to enable lens masses to be determined where applicable.\n",
        "\n",
        "It should be noted that in the simulated data, the inertial frame of reference was defined with the $x$-axis increasing from the binary center of mass towards the less massive lens at `t0`, the time of closest approach to the center of mass. If viewed from the solar system barycenter, the inertial frame moves at the relative velocity `vlens_CoM - vobserver(t0)`. The inclination of the orbit is a counter-clockwise rotation about the $x$-axis. $\\alpha$ is the angle that the source trajectory made with the $x$-axis (if parallax was 0). Where finite source effects were significant, a linear limb darkening law was applied.\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> What lightcurve models are included in this notebook? </font>\n",
        "\n",
        "The first data challenge contained the following kinds of lightcurves:\n",
        "\n",
        "* Cataclysmic Variable Star (CVS) false positive\n",
        "* Single lens microlensing events\n",
        "* Binary lens microlensing events\n",
        "\n",
        "You can expect a much broader set in the current data challenge with the addition of binary source microlensing events and more higher-order effects, including parallax, lens orbital motion and Xallarap. Higher-order effects included in the first data challenge, which are still included in this data challenge, are finite source effects and ... .\n",
        "\n",
        "Additional data types included in Data challenge 2 include:\n",
        "* Astrometric timeseries\n",
        "* Image postage stamps\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> What fitting codes are used in this notebook? </font>\n",
        "\n",
        "Some open source microlensing codes:\n",
        "\n",
        "| Name | Notes | Maintained | Link | Covered here |\n",
        "| :-: | :-: | :-: | :-: | :-: |\n",
        "| MuLensModel | User friendly, single- and binary-lens fitting code. | Yes ([Poleski](https://github.com/rpoleski)) | [GitHub](https://github.com/rpoleski/MulensModel) | [Yes](#mulensmodel) |\n",
        "| BAGEL | Incorporates photometric and astrometric microlensing. | Yes ([Moving Universe Lab](https://github.com/MovingUniverseLab)) | [GitHub](https://github.com/MovingUniverseLab/BAGLE_Microlensing) | [No](#bagel) |\n",
        "| VBMicroelensing | A more general version of the binary-lens code [VBBL](https://github.com/valboz/VBBinaryLensing). VBMicrolensing <br>is a tool for efficient computation in gravitational microlensing events <br>using the advanced contour integration method, supporting single, binary <br>and multiple lenses. | Yes ([Bozza](https://github.com/valboz)) | [GitHub](https://github.com/valboz/VBMicrolensing) | [No](#vbmicrolensing)\n",
        "| pyLIMA | pyLIMA is the first open source software for modeling microlensing <br>events. It should be flexible enough to handle your data and fit it. You can <br>also practice by simulating events. Useful for space-based observations. | Yes ([Bachelet](https://github.com/ebachelet)) | [GitHub](https://github.com/ebachelet/pyLIMA) | [Yes](#pylima) |\n",
        "| RTModel | Hands-off model fitting with built in model <i>\"interpretation\"</i> <br>(e.g. determing single-lens vs binary-lens arrangement) | Yes ([Bozza](https://github.com/valboz)) | [Github](https://github.com/valboz/RTModel) | [Yes](RTModel) |\n",
        "| eesunhong | No general description found. <br>See [Bennett and Rhie (1996)](https://ui.adsabs.harvard.edu/abs/1996ApJ...472..660B/abstract) and [Bennett (2010)](https://ui.adsabs.harvard.edu/abs/2010ApJ...716.1408B/abstract) | Yes ([Bennett]()) | [GitHub](https://github.com/golmschenk/eesunhong) | No |\n",
        "| pyLIMASS | Addition to pyLIMA for estimating physical properties of the lens <br>system. See [Bachelet, Hundertmark, and Calchi Novati (2024)](https://ui.adsabs.harvard.edu/abs/2024AJ....168...24B/abstract) | Yes ([Bachelet](https://github.com/ebachelet)) | [GitHub](https://github.com/ebachelet/pyLIMA/tree/master/pyLIMA/pyLIMASS) | [Yes](#pylima) |\n",
        "| popclass | Provides a flexible, probabilistic framework for classifying the lens <br>of a gravitational microlensing event. | Yes ([LLNL](https://github.com/LLNL)) | [GitHub](https://github.com/LLNL/popclass) | [Yes](#popclass) |\n",
        "| muLAn | Designed for fitting Roman microlensing lightcurve data. | No ([Cassan](https://github.com/ArnaudCassan)/[Ranc](https://github.com/clementranc)) | [GitHub](https://github.com/muLAn-project/muLAn) | [No](#muLAN) |\n",
        "| triplelens | Calculates light curves and image positions for triple microlensing <br>systems. (When the mass ratio is small (below ~ 1e-5), the solutions <br>from the lens equation solver are more accurate when the origin <br>of the coordinate system is set to be close to the smallest mass.) | No ([Kuang](https://github.com/rkkuang)) | [GitHub](https://github.com/rkkuang/triplelens) | No |\n",
        "| SingleLensFitter | Fits single lens events with finite source effects | No ([Albrow](https://github.com/MichaelDAlbrow)) | [GitHub](https://github.com/MichaelDAlbrow/SingleLensFitter) | No |\n",
        "<br>\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> Microlensing learning resources </font>\n",
        "\n",
        "* [RGES-PIT Minicourse](https://rges-pit.org/outreach_mini_landing/)\n",
        "\n",
        "  The RGES PIT has developed a microlensing mini course for select students to participate in various Roman-related lectures and activities during the Summer of 2025. The virtual lectures were held in mid May 2025; you can find lecture materials, recordings, and assignments.\n",
        "\n",
        "* [Microlensing Source](https://www.microlensing-source.org/)\n",
        "\n",
        "  Microlensing Source is a resource center for all aspects of gravitational microlensing. It aims to make microlensing more accessible for anyone with an interest in the subject - including students considering a career in the field, citizen scientists and those looking for a ready reference.\n",
        "\n",
        "* [The Microlenser's Guide to the Galaxy](https://github.com/AmberLee2427/TheMicrolensersGuideToTheGalaxy)\n",
        "\n",
        "  The goal of this project is to create an all-encompassing collection of Jupyter notebooks—your trusty companions for engaging exercises related to microlensing. Through these notebooks, the insights and experiences of microlensing veterins can light your path as you embark on your journey of discovery and exploration through scientific research.\n",
        "\n",
        "* [2017 Sagan Workshop](http://nexsci.caltech.edu/workshop/2017/)\n",
        "\n",
        "  The 2017 Sagan Summer Workshop focus on searching for planets with Roman (previously known as WFIRST) microlensing. Leaders in the field will discussed the importance of microlensing to understanding planetary populations and demographics, especially beyond the snow line. They reviewed the microlensing method, both in the context of current capabilities and the future Roman microlensing survey. In addition, speakers addressed the broad potential of the Romans's Wide Field Imaging microlensing survey for (non-microlensing) science in the galactic bulge. Attendees participated in hands-on group projects related to the Roman microlensing planet survey and had the opportunity to present their own work through short presentations (research POPs) and posters.\n",
        "  The recordings from this workshop can be found [here](https://www.youtube.com/watch?v=QPfKucBb9B8&list=PLIbTYGsIVYthWRS14eCEK8SK9IOTcaYsf)\n",
        "\n",
        "* [Glossary of Terms](https://www.microlensing-source.org/glossary/)\n",
        "\n",
        "  This glossary, from Microlensing Source, is intended as a quick reference, particularly to disambiguate the different symbol sets used by different authors over time. Interested readers are referred to the references at the bottom for a full discussion, especially Skowron et al. (2011), and to the Learning Resources menu.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5OeZIbHv2eB"
      },
      "source": [
        "## <font face=\"Helvetica\" size=\"6\"> Collecting the Data </font>\n",
        "\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "You can collapse this section and blindly click the play button to run all 13 cells in this section, which will download the data and organize it into dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I42DQYfDYPo3",
        "outputId": "87c385cb-ea30-4981-9aa9-6978fa6a6aaa"
      },
      "outputs": [],
      "source": [
        "#@title Cloning the GitHub repository\n",
        "\n",
        "# clone the microlensing data challenge repo\n",
        "!git clone https://github.com/microlensing-data-challenge/data-challenge-1.git\n",
        "\n",
        "# Extract the lightcurve files\n",
        "!tar -xzvf data-challenge-1/lc.tar.gz -C data-challenge-1/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xMN-qBOwg9B"
      },
      "outputs": [],
      "source": [
        "#@title Displaying PDFs in a notebook (browser dependent compatability)\n",
        "#from IPython.display import IFrame\n",
        "#\n",
        "## Assuming the PDF is in the current working directory\n",
        "#pdf_path = \"data-challenge-1/Answers/DataChallenge2019_Summary_byJenniferYee.pdf\"\n",
        "#\n",
        "## Display the PDF using IFrame\n",
        "#IFrame(pdf_path, width=800, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXVB6stgwrNA"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> Single Lens Events </font>\n",
        "\n",
        "This dataset includes 293 lightcurve, 74 of which are single lens events. We can cheat a little and specifically pull out the events that we know to be single lenses, keeping the challenge tractable for completion within the hour, with the added benefit of making the strangley organized `master_file.txt` easier to wrangle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zvjvl6nw3hx",
        "outputId": "e32adf1b-7584-4cc6-e599-65642bff3a00"
      },
      "outputs": [],
      "source": [
        "#@title Putting everything in a tidy data frame\n",
        "\n",
        "master_file = '/content/data-challenge-1/Answers/master_file.txt'\n",
        "header_file = '/content/data-challenge-1/Answers/wfirstColumnNumbers.txt'\n",
        "\n",
        "rows = []\n",
        "with open(master_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        # Skip empty lines or comment lines\n",
        "        if not line or line.startswith(\"#\"):\n",
        "            continue\n",
        "\n",
        "        tokens = line.split()  # split on whitespace\n",
        "        # Keep only single-lens events\n",
        "        if \"dcnormffp\" not in tokens:\n",
        "            continue\n",
        "\n",
        "        # Single-lens lines should have exactly 96 columns\n",
        "        if len(tokens) != 96:\n",
        "            continue\n",
        "\n",
        "        rows.append(tokens)\n",
        "\n",
        "df_sl = pd.DataFrame(rows)\n",
        "\n",
        "# make an array of zeros with 97 elements\n",
        "colnames_96 = np.zeros(96, dtype=object)\n",
        "\n",
        "# Read the header file\n",
        "with open(header_file, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        # Skip empty lines or comments\n",
        "        if not line or line.startswith('#'):\n",
        "            continue\n",
        "        # The second token is the 'name'\n",
        "        parts = line.split()\n",
        "        colnames_96[int(parts[0])] = parts[1]\n",
        "\n",
        "#For single lenses they are (***Note for these, the mass of the lens is given by the planet mass column, not the host mass column):\n",
        "#72 - unimportant\n",
        "#73 - N, number of consecutive W149 data points deviating by >=3 sigma from a flat line\n",
        "#74 - unimportant\n",
        "#75 - Delta chi^2 (relative to a flat line)\n",
        "#76-91 - unimportant\n",
        "#92 - simulated event type (dcnormffp = single lens or free-floating planet)\n",
        "#93 - unimportant (I think)\n",
        "#94 - lightcurve filename root\n",
        "#95 - Data challenge lightcurve number\n",
        "\n",
        "# Replace the column names in colnames_96\n",
        "colnames_96[73] = 'N'\n",
        "colnames_96[75] = 'Delta chi2'\n",
        "colnames_96[92] = 'sim type'\n",
        "colnames_96[94] = 'filename'\n",
        "colnames_96[95] = 'lc_number'\n",
        "\n",
        "# Make sure the column names are unique\n",
        "for i in range(94):\n",
        "    if colnames_96[i] == '|' or colnames_96[i] == 0:\n",
        "        colnames_96[i] = 'col_' + str(i)\n",
        "\n",
        "# Replace the column names in the data_frame\n",
        "df_sl.columns = colnames_96\n",
        "\n",
        "# Remove the dummy columns 'col_*'\n",
        "df_sl = df_sl.loc[:, ~df_sl.columns.str.startswith('col_')]\n",
        "\n",
        "df_sl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejq8QsYFxI-t"
      },
      "source": [
        "The last column in this data frame has the lightcurve number, which we can use to pick out only the lightcurves matching our single-lens event list, for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5OmYak-xGVK",
        "outputId": "b4eac558-b158-40ac-d52c-9d0e7426daca"
      },
      "outputs": [],
      "source": [
        "#@title Figuring out which files we want\n",
        "\n",
        "lc_number = df_sl['lc_number'].to_numpy()\n",
        "\n",
        "lc_file_path_format = 'data-challenge-1/lc/ulwdc1_XXX_filter.txt'\n",
        "\n",
        "lc_file_paths_W149 = [lc_file_path_format.replace('filter', 'W149')] * len(lc_number)\n",
        "lc_file_paths_Z087 = [lc_file_path_format.replace('filter', 'Z087')] * len(lc_number)\n",
        "\n",
        "# replace XXX, from the right, with the lc_number which is not necessarily of length 3\n",
        "lc_file_paths_W149 = [path.replace('XXX', str(num).zfill(3)) for path, num in zip(lc_file_paths_W149, lc_number)]\n",
        "lc_file_paths_Z087 = [path.replace('XXX', str(num).zfill(3)) for path, num in zip(lc_file_paths_Z087, lc_number)]\n",
        "\n",
        "df_sl['lc_file_path_W149'] = lc_file_paths_W149\n",
        "df_sl['lc_file_path_Z087'] = lc_file_paths_Z087\n",
        "\n",
        "df_sl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1nERAlwxIk6"
      },
      "source": [
        "There are a few pieces of information that may need to be known for each event that are not in the lightcurve files. These are stored in event_info.txt\n",
        "\n",
        "Columns: `\"Event_name\"` `\"Event_number\"` `\"RA_(deg)\"` `\"Dec_(deg)\"` `\"Distance\"` `\"A_W149\"` `\"sigma_A_W149\"` `\"A_Z087\"` `\"sigma_A_Z087\"`\n",
        "\n",
        "Distance, A_W149/Z087 are an estimate of the distance and extinction in each band of the red clump stars. sigma_A_W149/Z087 are dispersions in the extinction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX035n49xXov",
        "outputId": "ec86277a-6a5a-48a0-f6e3-7ce029052eb1"
      },
      "outputs": [],
      "source": [
        "#@title Event information data frame\n",
        "\n",
        "header = [\"Event_name\",\n",
        "          \"Event_number\",\n",
        "          \"RA_(deg)\",\n",
        "          \"Dec_(deg)\",\n",
        "          \"Distance\",\n",
        "          \"A_W149\",\n",
        "          \"sigma_A_W149\",\n",
        "          \"A_Z087\",\n",
        "          \"sigma_A_Z087\"\n",
        "]\n",
        "\n",
        "event_info = pd.read_csv('./data-challenge-1/event_info.txt', names=header, delim_whitespace=True)\n",
        "event_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYO43WegxcoQ",
        "outputId": "7975175b-a53f-486a-cd19-b1c6147e9ab2"
      },
      "outputs": [],
      "source": [
        "#@title Combining the two data frames\n",
        "\n",
        "# Convert 'lc_number' to numeric type before merging\n",
        "merged_sl_df = pd.merge(event_info, df_sl.astype({'lc_number': 'int64'}), left_on='Event_number', right_on='lc_number', how='inner')\n",
        "merged_sl_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XagtOAQx3Uh"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> Binary Lens Events </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n_RahyAx9YA"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> Triple Lens Events </font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbIYV9J3xh6h",
        "outputId": "92020c65-4001-4e11-9830-cad99ef42fa2"
      },
      "outputs": [],
      "source": [
        "try:  # this will only work on Colab.\n",
        "  sl_sheet = sheets.InteractiveSheet(df=merged_sl_df)\n",
        "  sl_sheet.show()\n",
        "  bl_sheet = sheets.InteractiveSheet(df=df_bl)\n",
        "  bl_sheet.show()\n",
        "except:\n",
        "  pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h96szZeOxYyb"
      },
      "source": [
        "Great - data successfully wrangled. Let's forget we ever had to live through that and move right along."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJmKF1cMyzho"
      },
      "source": [
        "## <font face=\"Helvetica\" size=\"6\"> Packages Covered in This Notebook </font>\n",
        "\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "<details>\n",
        "<summary><font face=\"Helvetica\" size=\"5\">1. MulensModel</font></summary>\n",
        "\n",
        "[Go to the `MulensModel` section](#mulensmodel)\n",
        "  * [1.1 Data objects](#11-data-objects)\n",
        "  * [1.2 Model objects](#12-model-objects)\n",
        "  * [1.3 Event objects](#13-event-objetcs)\n",
        "  * [1.4 Fitting](#fitting-framework)\n",
        "  * [1.5 Higher-order effects](#model-examples)\n",
        "    - [1S1L](#1s1l-example)\n",
        "    - [1S2L](#1s2l-example)\n",
        "    - [1S3L](#1s3l-example)\n",
        "    - [2S1L](#2s1l-example)\n",
        "    - [Parallax](#parallax-example)\n",
        "</details>\n",
        "<details>\n",
        "<summary><font face=\"Helvetica\" size=\"5\">2. pyLIMA</font></summary>\n",
        "\n",
        "[Go to the `pyLIMA` section](#pylima)\n",
        "  * [2.1 Installation and Setup](#pylima-installation)\n",
        "  * [2.2 Basic Usage](#pylima-basic)\n",
        "  * [2.3 Model Examples](#pylima-models)\n",
        "    - [2.3.1 Single Lens (PSPL)](#pylima-pspl)\n",
        "    - [2.3.2 Finite Source (FSPL)](#pylima-fspl)\n",
        "    - [2.3.3 Simulation Capabilities](#pylima-simulation)\n",
        "  * [2.4 Advanced Features](#pylima-advanced)\n",
        "    - [2.4.1 Custom Parameter Definitions](#pylima-custom-params)\n",
        "    - [2.4.2 Multiple Telescopes and Filters](#pylima-multiple-telescopes)\n",
        "  * [2.5 Performance and Best Practices](#pylima-performance)\n",
        "</details>\n",
        "<details>\n",
        "<summary><font face=\"Helvetica\" size=\"5\">4. RTModel</font></summary>\n",
        "\n",
        "[Go to the `RTModel` section](#rtmodel)\n",
        "  * [3.1 Installation and Setup](#rtmodel-installation)\n",
        "  * [3.2 Data Preparation](#rtmodel-data-prep)\n",
        "  * [3.3 Basic Usage](#rtmodel-basic-usage)\n",
        "  * [3.4 Understanding RTModel Output](#rtmodel-output)\n",
        "  * [3.5 Model Categories](#rtmodel-model-categories)\n",
        "  * [3.6 Visualization and Results](#rtmodel-visualization)\n",
        "  * [3.7 Advanced Features](#rtmodel-advanced)\n",
        "  * [3.8 RTModel vs Other Tools](#rtmodel-comparison)\n",
        "  * [3.9 Astrophotometric Fitting (RTModel v3.0)](#rtmodel-astrometric)\n",
        "    - [3.9.1 Astrophotometric Data Format](#rtmodel-astrometric-data)\n",
        "    - [3.9.2 Astrophotometric Parameters](#rtmodel-astrometric-parameters)\n",
        "    - [3.9.3 Astrophotometric Fitting](#rtmodel-astrometric-fitting)\n",
        "    - [3.9.4 Astrometric Visualization](#rtmodel-astrometric-visualization)\n",
        "    - [3.9.5 Applications and Use Cases](#rtmodel-astrometric-applications)\n",
        "</details>\n",
        "<details>\n",
        "<summary><font face=\"Helvetica\" size=\"5\">4. popclass</font></summary>\n",
        "\n",
        "[Go to the `popclass` section](#popclass)\n",
        "  * [4.1 Installation](#popclass-installation)\n",
        "  * [4.2 Basic Usage Example](#popclass-basic-usage)\n",
        "  * [4.3 How It Works](#popclass-theory)\n",
        "  * [4.4 Population Models](#popclass-models)\n",
        "  * [4.5 Advanced Features](#popclass-advanced)\n",
        "  * [4.6 Best Practices & Caveats](#popclass-best-practices)\n",
        "  * [4.7 Further Reading](#popclass-references)\n",
        "</details>\n",
        "<!-- \n",
        "<details>\n",
        "<summary><font face=\"Helvetica\" size=\"5\">5. BAGEL</font></summary> -->\n",
        "<!--\n",
        "[Go to the `BAGEL` section](#bagel)\n",
        "</details>\n",
        "<details>\n",
        "<summary><font face=\"Helvetica\" size=\"5\">6. MuLAN</font></summary> -->\n",
        "<!--\n",
        "[Go to the `MuLAN` section](#mulan)\n",
        "</details> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svCZcYocGgmW"
      },
      "source": [
        "## <font face=\"Helvetica\" size=\"6\"> 1. MulensModel </font>\n",
        "\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "At the time of writting this notebook there was an unresolved bug in the `MulensModel` package. The result is that models with finte source effects will raise an error for missing files. You can run all cells in the following subsection to correct it. But basically we need to download the repo's data directory and replace it in the package file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYefiE7-HPSU"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"5\"> MulensModel package fix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF98XaXkJLJt",
        "outputId": "425532e3-e0ed-4f94-9864-dc0a0c8dcc67"
      },
      "outputs": [],
      "source": [
        "#@title Installing the package\n",
        "reset_cwd()\n",
        "!pip install MulensModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "O4r2waMHJOfy"
      },
      "outputs": [],
      "source": [
        "#@title Importing the package\n",
        "import MulensModel as mm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzz9eSGVIyTB",
        "outputId": "73e94b87-4ab6-4b3b-d29a-ba9c77553458"
      },
      "outputs": [],
      "source": [
        "#@title Clearing the old `data` directory/file\n",
        "# check this box if you would like to replace the data folder\n",
        "replace_data_folder = False #@param {type:\"boolean\"}\n",
        "skip_next_cell = False\n",
        "\n",
        "mulensmodel_dir = os.path.dirname(mm.__file__)\n",
        "data_file_path = os.path.join(mulensmodel_dir, 'data')\n",
        "\n",
        "if os.path.exists(data_file_path):\n",
        "  # make sure we have permissions to delete the data file\n",
        "  os.chmod(os.path.join(mulensmodel_dir, \"data\"), 0o777)\n",
        "\n",
        "  if os.path.isfile(data_file_path):\n",
        "    os.remove(data_file_path)\n",
        "    print(f\"Removed 'data' file from {mulensmodel_dir}\")\n",
        "  elif replace_data_folder:\n",
        "    shutil.rmtree(data_file_path)\n",
        "    print(f\"Removed 'data' directory from {mulensmodel_dir}\")\n",
        "  else:\n",
        "    skip_next_cell = True\n",
        "    print(\"\"\"MulensModel build looks correct. If it is not working, try selecting\n",
        "     `replace_data_folder` and running the subsection again.\"\"\")\n",
        "else:\n",
        "  print(f\"No 'data' file or directory found in {mulensmodel_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oTi5AaPPLTov"
      },
      "outputs": [],
      "source": [
        "#@title Getting the data tree and cleaning up\n",
        "if not skip_next_cell:\n",
        "    # change the working directory to the mulensmodel_dir directory\n",
        "    os.chdir(mulensmodel_dir)\n",
        "\n",
        "    # starting from a fresh clone\n",
        "    if os.path.exists(os.path.join(mulensmodel_dir, \"MulensModel\")):\n",
        "        # remove the MulensModel repository\n",
        "        !chmod -R u+w MulensModel\n",
        "        !rm -r MulensModel\n",
        "\n",
        "    # clone the MulensModel repository\n",
        "    !git clone https://github.com/rpoleski/MulensModel.git\n",
        "\n",
        "    # move the data directory to the mulensmodel_dir directory\n",
        "    !mv MulensModel/data ./\n",
        "\n",
        "    # remove the MulensModel repository\n",
        "    !chmod -R u+w MulensModel\n",
        "    !rm -r MulensModel\n",
        "\n",
        "    # change the working directory back to the original directory\n",
        "    reset_cwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuT7Llu-FtfS"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 1.1 Data Objects </font>\n",
        "\n",
        "When fitting Roman data with `MulensModel`, we need to make a minor adjustment to our model for data that is not ground based. In `MulensModel` this simply means adding the following keyword to the data object initialization: `ephemerides_file=PATH_TO_THE_FILE`.\n",
        "\n",
        "> Instructions specific to this data set for `MulensModel` are given [here](https://github.com/rpoleski/MulensModel/blob/master/documents/data_challenge.md).\n",
        "\n",
        "Most of the data for these events is in the W147 band, so we make the very reasonable decision to just fit those data and not have to deal with mutliple data sets with different $F_\\textrm{S}$ and $F_\\textrm{B}$ values. That should speen up our fits too. If we wanted to find the color of the source star at a later date we could fit just the flux parameters and leave the microlensing-model parameters fixed (as described in [this notebook](https://github.com/AmberLee2427/TheMicrolensersGuideToTheGalaxy/blob/3b783495eb9a916ee9670a0347c9325f6a5b0a21/Notebooks/SingleLens.ipynb)) using a linear regression, which would a fraction of a second per event."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYLdgDpqTgZ8"
      },
      "outputs": [],
      "source": [
        "#@title Collecting the relevent meta data\n",
        "data_file = merged_sl_df['lc_file_path_W149'][0]\n",
        "ra = merged_sl_df['RA_(deg)'][0]\n",
        "dec = merged_sl_df['Dec_(deg)'][0]\n",
        "print(ra, dec)\n",
        "\n",
        "# convert decimal ra and dec in degrees to \"17h57m16.56s -29d05m20.04s\"\n",
        "coord = SkyCoord(ra=ra * u.deg, dec=dec * u.deg, frame='icrs')\n",
        "hms_dms_string = coord.to_string('hmsdms')\n",
        "print(f\"SkyCoord default: {hms_dms_string}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q1arR-nGW4u"
      },
      "outputs": [],
      "source": [
        "#@title Example L2 `Data` object\n",
        "\n",
        "# Here is the main difference for space data - we provide the ephemeris for Roman:\n",
        "EPHEM_FILE = 'data-challenge-1/wfirst_ephemeris_W149.txt'\n",
        "data_Roman_W149 = mm.MulensData(file_name=data_file,\n",
        "                                phot_fmt='mag',\n",
        "                                ephemerides_file=EPHEM_FILE,\n",
        "                                plot_properties={'color': '#a859e4',\n",
        "                                                 'label': 'Roman W149'\n",
        "                                                 },\n",
        "                                bandpass='H'\n",
        "                               )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5fC2Ks5Ts0N"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 1.2 Model Objects </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b3IdJjQT3Qw"
      },
      "outputs": [],
      "source": [
        "#@title Collecting the meta data\n",
        "# split the parallax equally out of a lack of better ideas\n",
        "pi_E_E = np.sqrt(float(merged_sl_df['piE'][0])**2 / 2.0)\n",
        "pi_E_N = pi_E_E * 1.0\n",
        "\n",
        "t_0 = float(merged_sl_df['t0'][0])\n",
        "# Annoyingly, t_0 is in simulation time not HJD so we need to do a conversion\n",
        "# (https://github.com/microlensing-data-challenge/evaluation_code/blob/master/parse_table1.py)\n",
        "# line 402\n",
        "t_0 = t_0 + 2458234.0  # simulation 0 time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR9_hxDaUAuN"
      },
      "outputs": [],
      "source": [
        "#@title Making a dictionary of the guess parameters\n",
        "# Let's just tidy the \"guess\" parameters up into a dictionary, for easy accesss\n",
        "params = dict()\n",
        "parameters_to_fit = [\"t_0\", \"u_0\", \"t_E\", \"rho\", \"pi_E_N\", \"pi_E_E\"]\n",
        "params['t_0'] = t_0 * 1.0\n",
        "params['t_0_par'] = t_0 * 1.0\n",
        "params['u_0'] = float(merged_sl_df['u0'][0]) * 1.1\n",
        "params['t_E'] = float(merged_sl_df['tE'][0]) * 1.1\n",
        "params['rho'] = float(merged_sl_df['rhos'][0]) * 1.1\n",
        "params['pi_E_N'] = pi_E_E\n",
        "params['pi_E_E'] = pi_E_E"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm3syBy5Sjc6"
      },
      "outputs": [],
      "source": [
        "#@title Example `Model` object\n",
        "\n",
        "# If we are using parallax, it is also important that we provide the event\n",
        "# coordinates, or MulensModel can't do necessary calculations\n",
        "Roman_model = mm.Model({**params},\n",
        "                        coords=coord,\n",
        "                        ephemerides_file=EPHEM_FILE\n",
        "                       )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1SlSsxvU1al"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 1.3 Event Objects </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VKhMYSTTDok"
      },
      "outputs": [],
      "source": [
        "#@title Example `Event` object\n",
        "\n",
        "Roman_event = mm.Event(datasets=data_Roman_W149, model=Roman_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fitting_framework"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 1.4 Fitting Framework </font>\n",
        "\n",
        "Now we'll set up a comprehensive fitting framework that can handle different model types. We'll use `emcee` for MCMC sampling and demonstrate various microlensing models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fitting_setup"
      },
      "outputs": [],
      "source": [
        "#@title Import additional fitting tools\n",
        "import emcee\n",
        "from scipy.optimize import minimize\n",
        "import corner\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "likelihood_function"
      },
      "outputs": [],
      "source": [
        "#@title Define likelihood function for fitting\n",
        "def log_likelihood(theta, event, parameters_to_fit):\n",
        "    \"\"\"Log likelihood function for emcee\"\"\"\n",
        "    try:\n",
        "        # Update model parameters\n",
        "        for i, param_name in enumerate(parameters_to_fit):\n",
        "            if param_name == 'log_rho':\n",
        "                # Convert log_rho back to rho\n",
        "                event.model.parameters.rho = 10**theta[i]\n",
        "            else:\n",
        "                setattr(event.model.parameters, param_name, theta[i])\n",
        "        \n",
        "        # Fit the fluxes given the current model parameters\n",
        "        event.fit_fluxes()\n",
        "        \n",
        "        # Get the source and blend fluxes\n",
        "        ([F_S], F_B) = event.get_flux_for_dataset(event.datasets[0])\n",
        "        \n",
        "        # Calculate chi-squared\n",
        "        chi2 = event.get_chi2()\n",
        "        \n",
        "        # Add flux priors if needed (optional)\n",
        "        penalty = 0.0\n",
        "        if F_B <= 0:\n",
        "            penalty = ((F_B / 100)**2)  # Penalize negative blend flux\n",
        "        if F_S <= 0 or (F_S + F_B) <= 0:\n",
        "            return -np.inf  # Return inf if fluxes are non-physical\n",
        "        \n",
        "        return -0.5 * chi2 - penalty\n",
        "    except:\n",
        "        return -np.inf\n",
        "\n",
        "def run_mcmc_fit(event, parameters_to_fit, initial_guess, nwalkers=32, nsteps=1000, burnin=200):\n",
        "    \"\"\"Run MCMC fitting with emcee\"\"\"\n",
        "    ndim = len(parameters_to_fit)\n",
        "    \n",
        "    # Initial positions (slightly perturbed from initial guess)\n",
        "    pos = np.array(initial_guess) + 1e-4 * np.random.randn(nwalkers, ndim)\n",
        "    \n",
        "    # Use ThreadPool for notebook compatibility\n",
        "    n_cores = mp.cpu_count()\n",
        "    print(f\"Using {n_cores} threads for MCMC\")\n",
        "    \n",
        "    with ThreadPool(processes=n_cores) as pool:\n",
        "        sampler = emcee.EnsembleSampler(\n",
        "            nwalkers, ndim, log_likelihood, \n",
        "            args=(event, parameters_to_fit),\n",
        "            pool=pool\n",
        "        )\n",
        "        sampler.run_mcmc(pos, nsteps, progress=True)\n",
        "    \n",
        "    return sampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_examples"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 1.5 Model Examples </font>\n",
        "\n",
        "Now let's demonstrate different microlensing models. We'll start with simple models and work our way up to more complex ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s1l_example"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 1.5.1 Single Lens Single Source (1S1L) with Finite Source Effects </font>\n",
        "\n",
        "Let's start with a simple single lens model that includes finite source effects (ρ parameter)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s1l_setup"
      },
      "outputs": [],
      "source": [
        "#@title 1S1L Model Setup\n",
        "# Select a single lens event for demonstration\n",
        "event_idx = 0  # First single lens event\n",
        "data_file = merged_sl_df['lc_file_path_W149'][event_idx]\n",
        "ra = merged_sl_df['RA_(deg)'][event_idx]\n",
        "dec = merged_sl_df['Dec_(deg)'][event_idx]\n",
        "coord = SkyCoord(ra=ra * u.deg, dec=dec * u.deg, frame='icrs')\n",
        "\n",
        "# Get truth parameters for comparison\n",
        "t_0_truth = float(merged_sl_df['t0'][event_idx]) + 2458234.0  # Convert to HJD\n",
        "u_0_truth = float(merged_sl_df['u0'][event_idx])\n",
        "t_E_truth = float(merged_sl_df['tE'][event_idx])\n",
        "rho_truth = float(merged_sl_df['rhos'][event_idx])\n",
        "\n",
        "print(f\"Event {event_idx}: Truth parameters\")\n",
        "print(f\"  t_0: {t_0_truth:.3f}\")\n",
        "print(f\"  u_0: {u_0_truth:.4f}\")\n",
        "print(f\"  t_E: {t_E_truth:.2f} days\")\n",
        "print(f\"  ρ: {rho_truth:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s1l_data"
      },
      "outputs": [],
      "source": [
        "#@title Load data for 1S1L example\n",
        "# Load the light curve data\n",
        "data_1s1l = mm.MulensData(file_name=data_file,\n",
        "                          phot_fmt='mag',\n",
        "                          ephemerides_file=EPHEM_FILE,\n",
        "                          plot_properties={'color': '#a859e4', 'label': 'Roman W149'},\n",
        "                          bandpass='H')\n",
        "\n",
        "# Create initial model with finite source effects\n",
        "params_1s1l = {\n",
        "    't_0': t_0_truth * 1.0,  # Start with truth values\n",
        "    'u_0': u_0_truth * 1.1,  # Slightly perturb\n",
        "    't_E': t_E_truth * 1.1,\n",
        "    'rho': rho_truth * 1.1,\n",
        "}\n",
        "\n",
        "model_1s1l = mm.Model(params_1s1l, coords=coord, ephemerides_file=EPHEM_FILE)\n",
        "event_1s1l = mm.Event(datasets=data_1s1l, model=model_1s1l)\n",
        "\n",
        "# Plot the initial model\n",
        "plt.figure(figsize=(12, 6))\n",
        "event_1s1l.plot_data()\n",
        "event_1s1l.plot_model(color='red', label='Initial Model')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(f\"1S1L Event {event_idx} - Initial Model\")\n",
        "plt.xlabel(\"HJD\")\n",
        "plt.ylabel(\"Magnitude\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s1l_fit"
      },
      "outputs": [],
      "source": [
        "#@title Fit 1S1L model with MCMC\n",
        "# Set up parameters to fit\n",
        "parameters_to_fit_1s1l = [\"t_0\", \"u_0\", \"t_E\", \"rho\"]\n",
        "initial_guess_1s1l = [t_0_truth, u_0_truth, t_E_truth, rho_truth]\n",
        "\n",
        "print(\"Starting 1S1L MCMC fit...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Run MCMC\n",
        "sampler_1s1l = run_mcmc_fit(event_1s1l, parameters_to_fit_1s1l, initial_guess_1s1l, \n",
        "                           nwalkers=32, nsteps=500, burnin=100)\n",
        "\n",
        "# Get results\n",
        "samples_1s1l = sampler_1s1l.chain[:, 100:, :].reshape((-1, 4))\n",
        "best_fit_1s1l = np.median(samples_1s1l, axis=0)\n",
        "uncertainties_1s1l = np.std(samples_1s1l, axis=0)\n",
        "\n",
        "fit_time = time.time() - start_time\n",
        "print(f\"\\n1S1L Fit completed in {fit_time:.1f} seconds\")\n",
        "print(\"\\nResults:\")\n",
        "for i, param in enumerate(parameters_to_fit_1s1l):\n",
        "    print(f\"  {param}: {best_fit_1s1l[i]:.6f} ± {uncertainties_1s1l[i]:.6f}\")\n",
        "\n",
        "# Compare with truth\n",
        "print(\"\\nComparison with truth:\")\n",
        "truth_values = [t_0_truth, u_0_truth, t_E_truth, rho_truth]\n",
        "for i, param in enumerate(parameters_to_fit_1s1l):\n",
        "    diff = best_fit_1s1l[i] - truth_values[i]\n",
        "    sigma_diff = abs(diff) / uncertainties_1s1l[i]\n",
        "    print(f\"  {param}: {diff:.6f} ({sigma_diff:.2f}σ)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s1l_results"
      },
      "outputs": [],
      "source": [
        "#@title Plot 1S1L results\n",
        "# Update model with best fit parameters\n",
        "for i, param_name in enumerate(parameters_to_fit_1s1l):\n",
        "    setattr(event_1s1l.model.parameters, param_name, best_fit_1s1l[i])\n",
        "event_1s1l.fit_fluxes()\n",
        "\n",
        "# Plot light curve with best fit\n",
        "plt.figure(figsize=(12, 6))\n",
        "event_1s1l.plot_data()\n",
        "event_1s1l.plot_model(color='red', label='Best Fit')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(f\"1S1L Event {event_idx} - Best Fit Model\")\n",
        "plt.xlabel(\"HJD\")\n",
        "plt.ylabel(\"Magnitude\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot corner plot\n",
        "fig = corner.corner(samples_1s1l, labels=parameters_to_fit_1s1l,\n",
        "                    truths=truth_values, quantiles=[0.16, 0.5, 0.84])\n",
        "plt.suptitle(f\"1S1L Event {event_idx} - Parameter Posteriors\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(12, 4))\n",
        "residuals = event_1s1l.get_residuals()\n",
        "plt.scatter(event_1s1l.datasets[0].time, residuals, alpha=0.6)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title(f\"1S1L Event {event_idx} - Residuals\")\n",
        "plt.xlabel(\"HJD\")\n",
        "plt.ylabel(\"Residuals (mag)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s2l_example"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 1.5.2 Single Source Binary Lens (1S2L) with Finite Source Effects </font>\n",
        "\n",
        "Now let's demonstrate a binary lens model. We'll use a simulated binary lens event to show how to fit for the additional parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s2l_setup"
      },
      "outputs": [],
      "source": [
        "#@title 1S2L Model Setup\n",
        "# For this example, we'll create a simulated binary lens event\n",
        "# In practice, you would load real binary lens data\n",
        "\n",
        "# Create synthetic binary lens data\n",
        "def create_binary_lens_data(t_0=2459123.5, u_0=0.1, t_E=25.0, q=0.1, s=1.2, alpha=45.0, rho=0.001):\n",
        "    \"\"\"Create synthetic binary lens light curve\"\"\"\n",
        "    # Time array around the event\n",
        "    t = np.linspace(t_0 - 2*t_E, t_0 + 2*t_E, 200)\n",
        "    \n",
        "    # Create binary lens model\n",
        "    params = {\n",
        "        't_0': t_0,\n",
        "        'u_0': u_0,\n",
        "        't_E': t_E,\n",
        "        'q': q,  # mass ratio\n",
        "        's': s,  # separation\n",
        "        'alpha': alpha,  # angle\n",
        "        'rho': rho  # finite source\n",
        "    }\n",
        "    \n",
        "    model = mm.Model(params, coords=coord, ephemerides_file=EPHEM_FILE)\n",
        "    \n",
        "    # Calculate magnification\n",
        "    magnification = model.get_magnification(t)\n",
        "    \n",
        "    # Add noise\n",
        "    mag_0 = 18.0  # baseline magnitude\n",
        "    mag = mag_0 - 2.5 * np.log10(magnification)\n",
        "    mag_err = 0.01 * np.ones_like(mag)  # 1% photometric error\n",
        "    \n",
        "    # Add some scatter\n",
        "    mag += np.random.normal(0, mag_err)\n",
        "    \n",
        "    return t, mag, mag_err, params\n",
        "\n",
        "# Create the synthetic data\n",
        "t_binary, mag_binary, mag_err_binary, truth_params_binary = create_binary_lens_data()\n",
        "\n",
        "print(\"Synthetic 1S2L Event - Truth Parameters:\")\n",
        "for key, value in truth_params_binary.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s2l_data"
      },
      "outputs": [],
      "source": [
        "#@title Create 1S2L data object\n",
        "# Create data object from synthetic data\n",
        "data_list = [t_binary, mag_binary, mag_err_binary]\n",
        "data_1s2l = mm.MulensData(data_list=data_list,\n",
        "                          phot_fmt='mag',\n",
        "                          ephemerides_file=EPHEM_FILE,\n",
        "                          plot_properties={'color': '#a859e4', 'label': 'Synthetic Binary'},\n",
        "                          bandpass='H')\n",
        "\n",
        "# Create initial model (slightly perturbed from truth)\n",
        "params_1s2l = {k: v * (1.0 + 0.1 * np.random.randn()) for k, v in truth_params_binary.items()}\n",
        "model_1s2l = mm.Model(params_1s2l, coords=coord, ephemerides_file=EPHEM_FILE)\n",
        "event_1s2l = mm.Event(datasets=data_1s2l, model=model_1s2l)\n",
        "\n",
        "# Plot the initial model\n",
        "plt.figure(figsize=(12, 6))\n",
        "event_1s2l.plot_data()\n",
        "event_1s2l.plot_model(color='red', label='Initial Model')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"1S2L Synthetic Event - Initial Model\")\n",
        "plt.xlabel(\"HJD\")\n",
        "plt.ylabel(\"Magnitude\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot caustic structure\n",
        "plt.figure(figsize=(8, 8))\n",
        "event_1s2l.plot_caustics()\n",
        "plt.title(\"1S2L Event - Caustic Structure\")\n",
        "plt.xlabel(\"x (Einstein radii)\")\n",
        "plt.ylabel(\"y (Einstein radii)\")\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s2l_fit"
      },
      "outputs": [],
      "source": [
        "#@title Fit 1S2L model with MCMC\n",
        "# Set up parameters to fit (excluding rho for simplicity in this example)\n",
        "parameters_to_fit_1s2l = [\"t_0\", \"u_0\", \"t_E\", \"q\", \"s\", \"alpha\"]\n",
        "initial_guess_1s2l = [params_1s2l[p] for p in parameters_to_fit_1s2l]\n",
        "\n",
        "print(\"Starting 1S2L MCMC fit...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Run MCMC with more walkers and steps for binary lens\n",
        "sampler_1s2l = run_mcmc_fit(event_1s2l, parameters_to_fit_1s2l, initial_guess_1s2l, \n",
        "                           nwalkers=64, nsteps=1000, burnin=200)\n",
        "\n",
        "# Get results\n",
        "samples_1s2l = sampler_1s2l.chain[:, 200:, :].reshape((-1, 6))\n",
        "best_fit_1s2l = np.median(samples_1s2l, axis=0)\n",
        "uncertainties_1s2l = np.std(samples_1s2l, axis=0)\n",
        "\n",
        "fit_time = time.time() - start_time\n",
        "print(f\"\\n1S2L Fit completed in {fit_time:.1f} seconds\")\n",
        "print(\"\\nResults:\")\n",
        "for i, param in enumerate(parameters_to_fit_1s2l):\n",
        "    print(f\"  {param}: {best_fit_1s2l[i]:.6f} ± {uncertainties_1s2l[i]:.6f}\")\n",
        "\n",
        "# Compare with truth\n",
        "print(\"\\nComparison with truth:\")\n",
        "truth_values_1s2l = [truth_params_binary[p] for p in parameters_to_fit_1s2l]\n",
        "for i, param in enumerate(parameters_to_fit_1s2l):\n",
        "    diff = best_fit_1s2l[i] - truth_values_1s2l[i]\n",
        "    sigma_diff = abs(diff) / uncertainties_1s2l[i]\n",
        "    print(f\"  {param}: {diff:.6f} ({sigma_diff:.2f}σ)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s2l_results"
      },
      "outputs": [],
      "source": [
        "#@title Plot 1S2L results\n",
        "# Update model with best fit parameters\n",
        "for i, param_name in enumerate(parameters_to_fit_1s2l):\n",
        "    setattr(event_1s2l.model.parameters, param_name, best_fit_1s2l[i])\n",
        "event_1s2l.fit_fluxes()\n",
        "\n",
        "# Plot light curve with best fit\n",
        "plt.figure(figsize=(12, 6))\n",
        "event_1s2l.plot_data()\n",
        "event_1s2l.plot_model(color='red', label='Best Fit')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"1S2L Synthetic Event - Best Fit Model\")\n",
        "plt.xlabel(\"HJD\")\n",
        "plt.ylabel(\"Magnitude\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot corner plot\n",
        "fig = corner.corner(samples_1s2l, labels=parameters_to_fit_1s2l,\n",
        "                    truths=truth_values_1s2l, quantiles=[0.16, 0.5, 0.84])\n",
        "plt.suptitle(\"1S2L Event - Parameter Posteriors\", y=1.02)\n",
        "plt.show()\n",
        "\n",
        "# Plot updated caustic structure\n",
        "plt.figure(figsize=(8, 8))\n",
        "event_1s2l.plot_caustics()\n",
        "plt.title(\"1S2L Event - Best Fit Caustic Structure\")\n",
        "plt.xlabel(\"x (Einstein radii)\")\n",
        "plt.ylabel(\"y (Einstein radii)\")\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s3l_example"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 1.5.3 Single Source Triple Lens (1S3L) </font>\n",
        "\n",
        "Triple lens systems are rare but important for detecting hierarchical planetary systems. Here we demonstrate how to set up a triple lens model.\n",
        "\n",
        "**Triple Lens Model Parameters:**\n",
        "- **Primary lens**: t_0, u_0, t_E\n",
        "- **First companion**: q1, s1, alpha1\n",
        "- **Second companion**: q2, s2, alpha2, phi\n",
        "- **Finite source**: rho\n",
        "- **Higher order**: pi_E_N, pi_E_E\n",
        "\n",
        "**Example Triple Lens Parameters (hierarchical system):**\n",
        "- t_0: 2459123.5\n",
        "- u_0: 0.1\n",
        "- t_E: 25.0\n",
        "- q1: 0.1 (First companion - planet)\n",
        "- s1: 1.2 (Separation of first companion)\n",
        "- alpha1: 45.0\n",
        "- q2: 0.01 (Second companion - moon)\n",
        "- s2: 0.1 (Separation of second companion relative to first)\n",
        "- alpha2: 30.0\n",
        "- phi: 60.0 (Angle between companions)\n",
        "- rho: 0.001 (Finite source)\n",
        "\n",
        "**Note**: This represents a hierarchical triple system with:\n",
        "- Primary lens (star)\n",
        "- First companion (planet at ~1.2 Einstein radii)\n",
        "- Second companion (moon at ~0.1 Einstein radii from planet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s3l_setup"
      },
      "outputs": [],
      "source": [
        "#@title 1S3L Model Setup\n",
        "# Note: MulensModel doesn't natively support triple lenses\n",
        "# We'll demonstrate the concept and show how to extend it\n",
        "\n",
        "# For triple lenses, we need additional parameters:\n",
        "# - q2: mass ratio of second companion\n",
        "# - s2: separation of second companion\n",
        "# - alpha2: angle of second companion\n",
        "# - phi: angle between the two companions\n",
        "\n",
        "print(\"Triple Lens Model Parameters:\")\n",
        "print(\"  Primary lens: t_0, u_0, t_E\")\n",
        "print(\"  First companion: q1, s1, alpha1\")\n",
        "print(\"  Second companion: q2, s2, alpha2, phi\")\n",
        "print(\"  Finite source: rho\")\n",
        "print(\"  Higher order: pi_E_N, pi_E_E\")\n",
        "\n",
        "# Example triple lens parameters (hierarchical system)\n",
        "triple_lens_params = {\n",
        "    't_0': 2459123.5,\n",
        "    'u_0': 0.1,\n",
        "    't_E': 25.0,\n",
        "    'q1': 0.1,    # First companion (planet)\n",
        "    's1': 1.2,    # Separation of first companion\n",
        "    'alpha1': 45.0,\n",
        "    'q2': 0.01,   # Second companion (moon)\n",
        "    's2': 0.1,    # Separation of second companion (relative to first)\n",
        "    'alpha2': 30.0,\n",
        "    'phi': 60.0,  # Angle between companions\n",
        "    'rho': 0.001  # Finite source\n",
        "}\n",
        "\n",
        "print(\"\\nExample Triple Lens Parameters:\")\n",
        "for key, value in triple_lens_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\nNote: This represents a hierarchical triple system with:\")\n",
        "print(\"  - Primary lens (star)\")\n",
        "print(\"  - First companion (planet at ~1.2 Einstein radii)\")\n",
        "print(\"  - Second companion (moon at ~0.1 Einstein radii from planet)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s3l_implementation"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> Triple Lens Implementation Notes </font>\n",
        "\n",
        "MulensModel doesn't natively support triple lenses, but here's how you could extend it:\n",
        "\n",
        "**Triple Lens Implementation Options:**\n",
        "\n",
        "**1. Use VBMicrolensing (covered in section 3):**\n",
        "- Native support for multiple lenses\n",
        "- More efficient for complex lens systems\n",
        "- Better suited for hierarchical systems\n",
        "\n",
        "**2. Extend MulensModel:**\n",
        "- Create custom model class\n",
        "- Implement triple lens magnification calculation\n",
        "- Requires significant development effort\n",
        "\n",
        "**3. Use triplelens package:**\n",
        "- Specialized for triple lens calculations\n",
        "- GitHub: https://github.com/rkkuang/triplelens\n",
        "- Limited integration with other tools\n",
        "\n",
        "**4. Approximate approach:**\n",
        "- Treat as hierarchical binary systems\n",
        "- Fit inner binary first, then outer binary\n",
        "- May miss some triple lens effects\n",
        "\n",
        "**Framework for Triple Lens Fitting:**\n",
        "```python\n",
        "parameters_to_fit_1s3l = [\n",
        "    't_0', 'u_0', 't_E',\n",
        "    'q1', 's1', 'alpha1',\n",
        "    'q2', 's2', 'alpha2', 'phi',\n",
        "    'rho'\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s1l_example"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 1.5.4 Binary Source Single Lens (2S1L) </font>\n",
        "\n",
        "Binary source events occur when the source star is actually a binary system. This can create distinctive light curve features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s1l_setup"
      },
      "outputs": [],
      "source": [
        "#@title 2S1L Model Setup\n",
        "# Binary source parameters:\n",
        "# - t_0, u_0, t_E: standard microlensing parameters\n",
        "# - q_S: flux ratio of source components\n",
        "# - rho_S: angular separation of source components\n",
        "# - theta_S: angle of source binary\n",
        "\n",
        "# Create synthetic binary source data\n",
        "def create_binary_source_data(t_0=2459123.5, u_0=0.1, t_E=25.0, q_S=0.8, rho_S=0.01, theta_S=45.0):\n",
        "    \"\"\"Create synthetic binary source light curve\"\"\"\n",
        "    # Time array around the event\n",
        "    t = np.linspace(t_0 - 2*t_E, t_0 + 2*t_E, 200)\n",
        "    \n",
        "    # Create binary source model\n",
        "    params = {\n",
        "        't_0': t_0,\n",
        "        'u_0': u_0,\n",
        "        't_E': t_E,\n",
        "        'q_S': q_S,      # flux ratio\n",
        "        'rho_S': rho_S,  # angular separation\n",
        "        'theta_S': theta_S  # angle\n",
        "    }\n",
        "    \n",
        "    # Note: MulensModel doesn't natively support binary sources\n",
        "    # This is a simplified approximation\n",
        "    model = mm.Model({'t_0': t_0, 'u_0': u_0, 't_E': t_E}, \n",
        "                     coords=coord, ephemerides_file=EPHEM_FILE)\n",
        "    \n",
        "    # Calculate magnification for single source\n",
        "    magnification = model.get_magnification(t)\n",
        "    \n",
        "    # Approximate binary source effect\n",
        "    # This is a simplified treatment - real binary sources are more complex\n",
        "    mag_0 = 18.0\n",
        "    mag = mag_0 - 2.5 * np.log10(magnification)\n",
        "    \n",
        "    # Add some binary source features (simplified)\n",
        "    # In reality, this would depend on the source binary orientation\n",
        "    binary_modulation = 0.1 * np.sin(2 * np.pi * (t - t_0) / t_E)\n",
        "    mag += binary_modulation\n",
        "    \n",
        "    mag_err = 0.01 * np.ones_like(mag)\n",
        "    mag += np.random.normal(0, mag_err)\n",
        "    \n",
        "    return t, mag, mag_err, params\n",
        "\n",
        "# Create the synthetic data\n",
        "t_binary_source, mag_binary_source, mag_err_binary_source, truth_params_binary_source = create_binary_source_data()\n",
        "\n",
        "print(\"Synthetic 2S1L Event - Truth Parameters:\")\n",
        "for key, value in truth_params_binary_source.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\nNote: This is a simplified binary source model.\")\n",
        "print(\"Real binary source events require more sophisticated modeling.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s1l_data"
      },
      "outputs": [],
      "source": [
        "#@title Create 2S1L data object\n",
        "# Create data object from synthetic data\n",
        "data_list_2s1l = [t_binary_source, mag_binary_source, mag_err_binary_source]\n",
        "data_2s1l = mm.MulensData(data_list=data_list_2s1l,\n",
        "                          phot_fmt='mag',\n",
        "                          ephemerides_file=EPHEM_FILE,\n",
        "                          plot_properties={'color': '#a859e4', 'label': 'Synthetic Binary Source'},\n",
        "                          bandpass='H')\n",
        "\n",
        "# Create initial model (treating as single source for now)\n",
        "params_2s1l = {\n",
        "    't_0': truth_params_binary_source['t_0'],\n",
        "    'u_0': truth_params_binary_source['u_0'],\n",
        "    't_E': truth_params_binary_source['t_E']\n",
        "}\n",
        "model_2s1l = mm.Model(params_2s1l, coords=coord, ephemerides_file=EPHEM_FILE)\n",
        "event_2s1l = mm.Event(datasets=data_2s1l, model=model_2s1l)\n",
        "\n",
        "# Plot the data\n",
        "plt.figure(figsize=(12, 6))\n",
        "event_2s1l.plot_data()\n",
        "event_2s1l.plot_model(color='red', label='Single Source Model')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"2S1L Synthetic Event - Data with Single Source Model\")\n",
        "plt.xlabel(\"HJD\")\n",
        "plt.ylabel(\"Magnitude\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Note: The residuals show the binary source signature.\")\n",
        "print(\"A proper binary source model would fit these features.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "parallax_example"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 1.5.5 Parallax Effects </font>\n",
        "\n",
        "Parallax effects are crucial for space-based observations like Roman. They allow us to measure the lens mass and distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "parallax_setup"
      },
      "outputs": [],
      "source": [
        "#@title Parallax Model Setup\n",
        "# Parallax parameters:\n",
        "# - pi_E_N: North component of parallax\n",
        "# - pi_E_E: East component of parallax\n",
        "# - t_0_par: reference time for parallax (usually same as t_0)\n",
        "\n",
        "# Let's use the original single lens event and add parallax\n",
        "print(\"Adding parallax to 1S1L event...\")\n",
        "\n",
        "# Create model with parallax\n",
        "params_parallax = {\n",
        "    't_0': t_0_truth,\n",
        "    'u_0': u_0_truth,\n",
        "    't_E': t_E_truth,\n",
        "    'rho': rho_truth,\n",
        "    'pi_E_N': 0.1,  # North component\n",
        "    'pi_E_E': 0.05, # East component\n",
        "    't_0_par': t_0_truth  # Reference time\n",
        "}\n",
        "\n",
        "model_parallax = mm.Model(params_parallax, coords=coord, ephemerides_file=EPHEM_FILE)\n",
        "event_parallax = mm.Event(datasets=data_1s1l, model=model_parallax)\n",
        "\n",
        "print(\"Parallax Model Parameters:\")\n",
        "for key, value in params_parallax.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Calculate parallax magnitude and direction\n",
        "pi_E_mag = np.sqrt(params_parallax['pi_E_N']**2 + params_parallax['pi_E_E']**2)\n",
        "pi_E_angle = np.arctan2(params_parallax['pi_E_N'], params_parallax['pi_E_E']) * 180 / np.pi\n",
        "print(f\"\\nParallax magnitude: {pi_E_mag:.3f}\")\n",
        "print(f\"Parallax angle: {pi_E_angle:.1f} degrees\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "parallax_fit"
      },
      "outputs": [],
      "source": [
        "#@title Fit parallax model\n",
        "# Set up parameters to fit including parallax\n",
        "parameters_to_fit_parallax = [\"t_0\", \"u_0\", \"t_E\", \"rho\", \"pi_E_N\", \"pi_E_E\"]\n",
        "initial_guess_parallax = [params_parallax[p] for p in parameters_to_fit_parallax]\n",
        "\n",
        "print(\"Starting parallax MCMC fit...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Run MCMC\n",
        "sampler_parallax = run_mcmc_fit(event_parallax, parameters_to_fit_parallax, initial_guess_parallax, \n",
        "                               nwalkers=64, nsteps=1000, burnin=200)\n",
        "\n",
        "# Get results\n",
        "samples_parallax = sampler_parallax.chain[:, 200:, :].reshape((-1, 6))\n",
        "best_fit_parallax = np.median(samples_parallax, axis=0)\n",
        "uncertainties_parallax = np.std(samples_parallax, axis=0)\n",
        "\n",
        "fit_time = time.time() - start_time\n",
        "print(f\"\\nParallax Fit completed in {fit_time:.1f} seconds\")\n",
        "print(\"\\nResults:\")\n",
        "for i, param in enumerate(parameters_to_fit_parallax):\n",
        "    print(f\"  {param}: {best_fit_parallax[i]:.6f} ± {uncertainties_parallax[i]:.6f}\")\n",
        "\n",
        "# Calculate derived quantities\n",
        "pi_E_mag_fit = np.sqrt(best_fit_parallax[4]**2 + best_fit_parallax[5]**2)\n",
        "pi_E_mag_err = np.sqrt((best_fit_parallax[4] * uncertainties_parallax[4])**2 + \n",
        "                       (best_fit_parallax[5] * uncertainties_parallax[5])**2) / pi_E_mag_fit\n",
        "print(f\"\\nDerived quantities:\")\n",
        "print(f\"  π_E magnitude: {pi_E_mag_fit:.3f} ± {pi_E_mag_err:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "parallax_results"
      },
      "outputs": [],
      "source": [
        "#@title Plot parallax results\n",
        "# Update model with best fit parameters\n",
        "for i, param_name in enumerate(parameters_to_fit_parallax):\n",
        "    setattr(event_parallax.model.parameters, param_name, best_fit_parallax[i])\n",
        "event_parallax.fit_fluxes()\n",
        "\n",
        "# Plot light curve with parallax model\n",
        "plt.figure(figsize=(12, 6))\n",
        "event_parallax.plot_data()\n",
        "event_parallax.plot_model(color='red', label='Parallax Model')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title(\"1S1L Event with Parallax - Best Fit Model\")\n",
        "plt.xlabel(\"HJD\")\n",
        "plt.ylabel(\"Magnitude\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot parallax parameter correlations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# pi_E_N vs pi_E_E\n",
        "axes[0,0].scatter(samples_parallax[:, 4], samples_parallax[:, 5], alpha=0.5)\n",
        "axes[0,0].set_xlabel('π_E_N')\n",
        "axes[0,0].set_ylabel('π_E_E')\n",
        "axes[0,0].set_title('Parallax Parameter Correlation')\n",
        "\n",
        "# pi_E magnitude distribution\n",
        "pi_E_mags = np.sqrt(samples_parallax[:, 4]**2 + samples_parallax[:, 5]**2)\n",
        "axes[0,1].hist(pi_E_mags, bins=30, alpha=0.7)\n",
        "axes[0,1].axvline(pi_E_mag_fit, color='red', linestyle='--', label=f'Best fit: {pi_E_mag_fit:.3f}')\n",
        "axes[0,1].set_xlabel('π_E magnitude')\n",
        "axes[0,1].set_ylabel('Count')\n",
        "axes[0,1].set_title('Parallax Magnitude Distribution')\n",
        "axes[0,1].legend()\n",
        "\n",
        "# t_E vs pi_E correlation\n",
        "axes[1,0].scatter(samples_parallax[:, 2], pi_E_mags, alpha=0.5)\n",
        "axes[1,0].set_xlabel('t_E (days)')\n",
        "axes[1,0].set_ylabel('π_E magnitude')\n",
        "axes[1,0].set_title('t_E vs π_E Correlation')\n",
        "\n",
        "# u_0 vs pi_E correlation\n",
        "axes[1,1].scatter(samples_parallax[:, 1], pi_E_mags, alpha=0.5)\n",
        "axes[1,1].set_xlabel('u_0')\n",
        "axes[1,1].set_ylabel('π_E magnitude')\n",
        "axes[1,1].set_title('u_0 vs π_E Correlation')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare with and without parallax\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(\"Without parallax:\")\n",
        "print(f\"  χ²: {event_1s1l.get_chi2():.2f}\")\n",
        "print(\"With parallax:\")\n",
        "print(f\"  χ²: {event_parallax.get_chi2():.2f}\")\n",
        "print(f\"  Improvement: {event_1s1l.get_chi2() - event_parallax.get_chi2():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "higher_order_summary"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 1.6 Higher-Order Effects Summary </font>\n",
        "\n",
        "We've demonstrated several key higher-order effects in microlensing:\n",
        "\n",
        "**Finite Source Effects (ρ):**\n",
        "- Important when the source star is resolved\n",
        "- Provides constraints on the lens mass and distance\n",
        "- Essential for accurate parameter estimation\n",
        "\n",
        "**Parallax Effects (π_E):**\n",
        "- Crucial for space-based observations\n",
        "- Enables mass and distance measurements\n",
        "- Breaks degeneracies in lens properties\n",
        "\n",
        "**Binary Lens Effects:**\n",
        "- Creates caustic structures\n",
        "- Enables planet detection\n",
        "- Requires more complex fitting procedures\n",
        "\n",
        "**Other Effects (not demonstrated):**\n",
        "- **Lens Orbital Motion:** Changes in binary lens separation over time\n",
        "- **Xallarap:** Source orbital motion effects\n",
        "- **Limb Darkening:** Non-uniform source brightness\n",
        "- **Astrometry:** Position measurements during events\n",
        "\n",
        "**Best Practices:**\n",
        "- Start with simple models and add complexity gradually\n",
        "- Use MCMC for complex parameter spaces\n",
        "- Validate results against known parameters when possible\n",
        "- Consider computational efficiency for large surveys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pylima_section"
      },
      "source": [
        "## <font face=\"Helvetica\" size=\"6\"> 2. pyLIMA </font>\n",
        "\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "pyLIMA (Python Lightcurve Interpretation and Microlensing Analysis) is the first open-source software for modeling microlensing events. It was developed by Etienne Bachelet and provides a flexible, modular framework for microlensing analysis. pyLIMA is particularly well-suited for space-based observations and offers advanced features like simulation capabilities and custom parameter definitions.\n",
        "\n",
        "**Key Features:**\n",
        "- Modular design with separate event, telescope, and model classes\n",
        "- Multiple fitting algorithms (LM, DE, MCMC, TRF)\n",
        "- Built-in simulation capabilities\n",
        "- Custom parameter definitions\n",
        "- Support for multiple telescopes and filters\n",
        "- Limb darkening and finite source effects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pylima_installation"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 2.1 Installation and Setup </font>\n",
        "\n",
        "pyLIMA can be installed via pip or conda. It has several dependencies including numpy, scipy, matplotlib, and astropy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylima_install"
      },
      "outputs": [],
      "source": [
        "#@title Install pyLIMA\n",
        "!pip install pyLIMA\n",
        "\n",
        "# Import pyLIMA modules\n",
        "from pyLIMA import event, telescopes\n",
        "from pyLIMA.models import PSPL_model, FSPL_model\n",
        "from pyLIMA.fits import LM_fit, DE_fit, MCMC_fit, TRF_fit\n",
        "from pyLIMA.outputs import pyLIMA_plots\n",
        "from pyLIMA.simulations import simulator\n",
        "from pyLIMA.models import pyLIMA_fancy_parameters\n",
        "\n",
        "print(\"pyLIMA installed and imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pylima_basic"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 2.2 Basic Usage </font>\n",
        "\n",
        "pyLIMA uses a modular approach with three main components:\n",
        "1. **Event**: Contains all information about the microlensing event\n",
        "2. **Telescopes**: Individual telescope data and properties\n",
        "3. **Models**: The microlensing model to fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylima_basic_setup"
      },
      "outputs": [],
      "source": [
        "#@title Basic pyLIMA Setup\n",
        "# Create a new event\n",
        "your_event = event.Event()\n",
        "your_event.name = 'pyLIMA Example Event'\n",
        "\n",
        "# Create synthetic data for demonstration\n",
        "# In practice, you would load real data from files\n",
        "np.random.seed(42)  # For reproducible results\n",
        "\n",
        "# Generate synthetic light curve data\n",
        "t0, u0, tE = 2459123.5, 0.1, 25.0\n",
        "times = np.linspace(t0 - 2*tE, t0 + 2*tE, 100)\n",
        "magnification = (u0**2 + 2) / (u0 * np.sqrt(u0**2 + 4))\n",
        "u = np.sqrt(u0**2 + ((times - t0) / tE)**2)\n",
        "A = (u**2 + 2) / (u * np.sqrt(u**2 + 4))\n",
        "\n",
        "# Add noise\n",
        "mag_0 = 18.0\n",
        "mag = mag_0 - 2.5 * np.log10(A)\n",
        "mag_err = 0.01 * np.ones_like(mag)\n",
        "mag += np.random.normal(0, mag_err)\n",
        "\n",
        "# Create telescope object\n",
        "lightcurve_data = np.column_stack([times, mag, mag_err])\n",
        "telescope_1 = telescopes.Telescope(\n",
        "    name='OGLE',\n",
        "    camera_filter='I',\n",
        "    lightcurve=lightcurve_data.astype(float),\n",
        "    lightcurve_names=['time', 'mag', 'err_mag'],\n",
        "    lightcurve_units=['JD', 'mag', 'mag']\n",
        ")\n",
        "\n",
        "# Add telescope to event\n",
        "your_event.telescopes.append(telescope_1)\n",
        "\n",
        "# Set survey telescope (for alignment)\n",
        "your_event.find_survey('OGLE')\n",
        "\n",
        "# Check event setup\n",
        "your_event.check_event()\n",
        "\n",
        "print(f\"Event '{your_event.name}' created with {len(your_event.telescopes)} telescope(s)\")\n",
        "print(f\"Data points: {len(telescope_1.lightcurve)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pylima_models"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 2.3 Model Examples </font>\n",
        "\n",
        "pyLIMA supports various microlensing models. Let's start with the basic Point Source Point Lens (PSPL) model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pylima_pspl"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 2.3.1 Single Lens (PSPL) Example </font>\n",
        "\n",
        "The PSPL model is the simplest microlensing model with parameters: `t_0`, `u_0`, `t_E`, and flux parameters for each telescope."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylima_pspl_model"
      },
      "outputs": [],
      "source": [
        "#@title PSPL Model Setup and Fitting\n",
        "# Create PSPL model\n",
        "pspl = PSPL_model.PSPLmodel(your_event)\n",
        "\n",
        "# Initialize fit with Levenberg-Marquardt algorithm\n",
        "my_fit = LM_fit.LMfit(pspl)\n",
        "\n",
        "# Show initial parameters\n",
        "print(\"Initial fit parameters:\")\n",
        "print(my_fit.fit_parameters)\n",
        "\n",
        "# Run the fit\n",
        "print(\"\\nRunning LM fit...\")\n",
        "my_fit.fit()\n",
        "my_fit.fit_outputs()\n",
        "\n",
        "# Display results\n",
        "print(\"\\nFit Results:\")\n",
        "print(my_fit.fit_results['best_model'])\n",
        "print(f\"\\nChi-squared: {my_fit.fit_results['chi2']:.2f}\")\n",
        "\n",
        "# Plot the results\n",
        "pyLIMA_plots.plot_lightcurves(pspl, my_fit.fit_results['best_model'])\n",
        "plt.title(\"pyLIMA PSPL Fit\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylima_pspl_mcmc"
      },
      "outputs": [],
      "source": [
        "#@title PSPL with MCMC\n",
        "# Use MCMC for better parameter estimation\n",
        "mcmc_fit = MCMC_fit.MCMCfit(pspl, rescale_photometry=True)\n",
        "\n",
        "# Use LM results as initial guess\n",
        "mcmc_fit.model_parameters_guess = my_fit.fit_results['best_model'][:3]  # t0, u0, tE\n",
        "\n",
        "print(\"Running MCMC fit (this may take a while)...\")\n",
        "mcmc_fit.fit()\n",
        "mcmc_fit.fit_outputs()\n",
        "\n",
        "# Get MCMC results\n",
        "MCMC_results = mcmc_fit.fit_results['MCMC_chains']\n",
        "burnin = 1000\n",
        "\n",
        "# Calculate statistics\n",
        "posterior_samples = MCMC_results[burnin:, :, :3]  # t0, u0, tE\n",
        "best_fit_mcmc = np.median(posterior_samples, axis=(0, 1))\n",
        "uncertainties_mcmc = np.std(posterior_samples, axis=(0, 1))\n",
        "\n",
        "print(\"\\nMCMC Results:\")\n",
        "param_names = ['t_0', 'u_0', 't_E']\n",
        "for i, name in enumerate(param_names):\n",
        "    print(f\"{name}: {best_fit_mcmc[i]:.4f} ± {uncertainties_mcmc[i]:.4f}\")\n",
        "\n",
        "# Compare with truth values\n",
        "print(\"\\nComparison with truth:\")\n",
        "truth_values = [t0, u0, tE]\n",
        "for i, name in enumerate(param_names):\n",
        "    diff = best_fit_mcmc[i] - truth_values[i]\n",
        "    sigma_diff = abs(diff) / uncertainties_mcmc[i]\n",
        "    print(f\"{name}: {diff:.4f} ({sigma_diff:.2f}σ)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pylima_fspl"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 2.3.2 Finite Source (FSPL) Example </font>\n",
        "\n",
        "The FSPL model includes finite source effects with the ρ parameter, which is crucial for high-magnification events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylima_fspl_model"
      },
      "outputs": [],
      "source": [
        "#@title FSPL Model with Finite Source Effects\n",
        "# Create FSPL model\n",
        "fspl = FSPL_model.FSPLmodel(your_event)\n",
        "\n",
        "# Set limb darkening coefficient\n",
        "your_event.telescopes[0].ld_gamma = 0.5\n",
        "\n",
        "# Use differential evolution for better exploration\n",
        "de_fit = DE_fit.DEfit(fspl, loss_function='chi2')\n",
        "\n",
        "print(\"Running DE fit for FSPL model...\")\n",
        "de_fit.fit()\n",
        "\n",
        "# Display results\n",
        "print(\"\\nFSPL Fit Results:\")\n",
        "print(de_fit.fit_results['best_model'])\n",
        "print(f\"\\nChi-squared: {de_fit.fit_results['chi2']:.2f}\")\n",
        "\n",
        "# Plot results\n",
        "pyLIMA_plots.plot_lightcurves(fspl, de_fit.fit_results['best_model'])\n",
        "plt.title(\"pyLIMA FSPL Fit with Finite Source Effects\")\n",
        "plt.show()\n",
        "\n",
        "# Compare PSPL vs FSPL\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(f\"PSPL χ²: {my_fit.fit_results['chi2']:.2f}\")\n",
        "print(f\"FSPL χ²: {de_fit.fit_results['chi2']:.2f}\")\n",
        "print(f\"Improvement: {my_fit.fit_results['chi2'] - de_fit.fit_results['chi2']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pylima_simulation"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 2.3.3 Simulation Capabilities </font>\n",
        "\n",
        "One of pyLIMA's unique features is its built-in simulation capabilities, allowing you to generate realistic microlensing light curves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylima_sim_setup"
      },
      "outputs": [],
      "source": [
        "#@title pyLIMA Simulation Example\n",
        "# Create a new event for simulation\n",
        "sim_event = event.Event(ra=270, dec=-30)\n",
        "sim_event.name = 'Simulated Event'\n",
        "\n",
        "# Create a realistic telescope with observing constraints\n",
        "CTIO_I = simulator.simulate_a_telescope(\n",
        "    name='CTIO_I',\n",
        "    time_start=2457365.5,\n",
        "    time_end=2457965.5,\n",
        "    sampling=4,\n",
        "    location='Earth',\n",
        "    camera_filter='I',\n",
        "    uniform_sampling=False,\n",
        "    altitude=1000,\n",
        "    longitude=-109.285399,\n",
        "    latitude=-27.130,\n",
        "    bad_weather_percentage=10.0/100,\n",
        "    moon_windows_avoidance=30,\n",
        "    minimum_alt=30,\n",
        "    astrometry=False\n",
        ")\n",
        "\n",
        "sim_event.telescopes.append(CTIO_I)\n",
        "sim_event.check_event()\n",
        "\n",
        "print(f\"Simulated telescope '{CTIO_I.name}' created\")\n",
        "print(f\"Observation period: {CTIO_I.time_start:.1f} to {CTIO_I.time_end:.1f}\")\n",
        "print(f\"Sampling: {CTIO_I.sampling} hours\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylima_sim_model"
      },
      "outputs": [],
      "source": [
        "#@title Generate Simulated Light Curve\n",
        "# Create PSPL model for simulation\n",
        "sim_pspl = PSPL_model.PSPLmodel(sim_event)\n",
        "\n",
        "# Generate random model parameters\n",
        "sim_parameters = simulator.simulate_microlensing_model_parameters(sim_pspl)\n",
        "print(\"Simulated Parameters:\")\n",
        "print(sim_parameters)\n",
        "\n",
        "# Convert to pyLIMA format\n",
        "pyLIMA_parameters = sim_pspl.compute_pyLIMA_parameters(sim_parameters)\n",
        "\n",
        "# Generate the light curve\n",
        "simulator.simulate_lightcurve(sim_pspl, pyLIMA_parameters)\n",
        "\n",
        "# Plot the simulated light curve\n",
        "pyLIMA_plots.plot_lightcurves(sim_pspl, sim_parameters)\n",
        "plt.title(\"pyLIMA Simulated Light Curve\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Generated {len(CTIO_I.lightcurve)} data points\")\n",
        "print(f\"Time range: {CTIO_I.lightcurve['time'].min():.1f} to {CTIO_I.lightcurve['time'].max():.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylima_sim_fit"
      },
      "outputs": [],
      "source": [
        "#@title Fit Simulated Data\n",
        "# Now fit the simulated data to recover parameters\n",
        "sim_fit = LM_fit.LMfit(sim_pspl)\n",
        "sim_fit.fit()\n",
        "sim_fit.fit_outputs()\n",
        "\n",
        "print(\"Fit Results vs Truth:\")\n",
        "print(\"Parameter | Truth    | Fit      | Difference\")\n",
        "print(\"---------|----------|----------|-----------\")\n",
        "\n",
        "param_names = list(sim_pspl.model_dictionnary.keys())\n",
        "for i, name in enumerate(param_names[:3]):  # t0, u0, tE\n",
        "    truth = sim_parameters[i]\n",
        "    fit = sim_fit.fit_results['best_model'][i]\n",
        "    diff = fit - truth\n",
        "    print(f\"{name:8} | {truth:8.4f} | {fit:8.4f} | {diff:+8.4f}\")\n",
        "\n",
        "print(f\"\\nChi-squared: {sim_fit.fit_results['chi2']:.2f}\")\n",
        "\n",
        "# Plot fit results\n",
        "pyLIMA_plots.plot_lightcurves(sim_pspl, sim_fit.fit_results['best_model'])\n",
        "plt.title(\"Fitting Simulated Data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pylima_advanced"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 2.4 Advanced Features </font>\n",
        "\n",
        "pyLIMA offers several advanced features including custom parameter definitions and multiple fitting algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pylima_custom_params"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 2.4.1 Custom Parameter Definitions </font>\n",
        "\n",
        "pyLIMA allows you to define custom parameter transformations, which can be useful for better sampling or physical interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylima_custom_setup"
      },
      "outputs": [],
      "source": [
        "#@title Custom Parameter Definitions\n",
        "# Define custom parameters: log_tE instead of tE\n",
        "class MyFancyParameters(object):\n",
        "    def __init__(self, fancy_parameters={'tE': 'log_tE'},\n",
        "                 fancy_boundaries={'log_tE': (0, 3)}):\n",
        "        self.fancy_parameters = fancy_parameters\n",
        "        self.fancy_boundaries = fancy_boundaries\n",
        "\n",
        "    def tE(self, fancy_params):\n",
        "        return 10**fancy_params['log_tE']\n",
        "\n",
        "    def log_tE(self, standard_params):\n",
        "        return np.log10(standard_params['tE'])\n",
        "\n",
        "# Create model with custom parameters\n",
        "my_pars = MyFancyParameters()\n",
        "fspl_custom = FSPL_model.FSPLmodel(your_event, fancy_parameters=my_pars)\n",
        "\n",
        "print(\"Custom parameter model created\")\n",
        "print(f\"Parameters: {fspl_custom.fit_parameters.keys()}\")\n",
        "print(f\"Custom boundaries: {my_pars.fancy_boundaries}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylima_custom_fit"
      },
      "outputs": [],
      "source": [
        "#@title Fit with Custom Parameters\n",
        "# Use Trust Region Reflective algorithm\n",
        "trf_fit = TRF_fit.TRFfit(fspl_custom)\n",
        "\n",
        "# Set initial guess (convert tE to log_tE)\n",
        "guess_parameters = [t0, u0, np.log10(tE), 0.001]  # t0, u0, log_tE, rho\n",
        "trf_fit.model_parameters_guess = guess_parameters\n",
        "\n",
        "print(\"Running TRF fit with custom parameters...\")\n",
        "trf_fit.fit()\n",
        "\n",
        "print(\"\\nFit Results (custom parameters):\")\n",
        "print(trf_fit.fit_results['best_model'])\n",
        "print(f\"\\nChi-squared: {trf_fit.fit_results['chi2']:.2f}\")\n",
        "\n",
        "# Convert back to standard parameters for comparison\n",
        "custom_params = trf_fit.fit_results['best_model']\n",
        "standard_tE = 10**custom_params[2]  # Convert log_tE back to tE\n",
        "print(f\"\\nConverted t_E: {standard_tE:.2f} days\")\n",
        "print(f\"Truth t_E: {tE:.2f} days\")\n",
        "print(f\"Difference: {standard_tE - tE:.2f} days\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pylima_multiple_telescopes"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 2.4.2 Multiple Telescopes and Filters </font>\n",
        "\n",
        "pyLIMA excels at handling data from multiple telescopes and filters, which is common in modern microlensing surveys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylima_multi_tel"
      },
      "outputs": [],
      "source": [
        "#@title Multiple Telescope Example\n",
        "# Create a new event with multiple telescopes\n",
        "multi_event = event.Event()\n",
        "multi_event.name = 'Multi-Telescope Event'\n",
        "\n",
        "# Generate data for multiple telescopes with different properties\n",
        "telescopes_data = []\n",
        "telescope_names = ['OGLE', 'MOA', 'LCO']\n",
        "filters = ['I', 'R', 'V']\n",
        "offsets = [0.0, 0.1, -0.05]  # Different zero points\n",
        "noise_levels = [0.01, 0.015, 0.02]  # Different noise levels\n",
        "\n",
        "for i, (name, filt, offset, noise) in enumerate(zip(telescope_names, filters, offsets, noise_levels)):\n",
        "    # Generate light curve with different properties\n",
        "    mag_tel = mag + offset + np.random.normal(0, noise, len(mag))\n",
        "    mag_err_tel = noise * np.ones_like(mag)\n",
        "    \n",
        "    lightcurve_data = np.column_stack([times, mag_tel, mag_err_tel])\n",
        "    \n",
        "    telescope = telescopes.Telescope(\n",
        "        name=name,\n",
        "        camera_filter=filt,\n",
        "        lightcurve=lightcurve_data.astype(float),\n",
        "        lightcurve_names=['time', 'mag', 'err_mag'],\n",
        "        lightcurve_units=['JD', 'mag', 'mag']\n",
        "    )\n",
        "    \n",
        "    multi_event.telescopes.append(telescope)\n",
        "    telescopes_data.append(lightcurve_data)\n",
        "\n",
        "# Set survey telescope\n",
        "multi_event.find_survey('OGLE')\n",
        "multi_event.check_event()\n",
        "\n",
        "print(f\"Created event with {len(multi_event.telescopes)} telescopes:\")\n",
        "for tel in multi_event.telescopes:\n",
        "    print(f\"  {tel.name} ({tel.camera_filter}-band): {len(tel.lightcurve)} points\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylima_multi_fit"
      },
      "outputs": [],
      "source": [
        "#@title Fit Multi-Telescope Data\n",
        "# Create PSPL model for multi-telescope data\n",
        "multi_pspl = PSPL_model.PSPLmodel(multi_event)\n",
        "\n",
        "# Fit with differential evolution\n",
        "multi_fit = DE_fit.DEfit(multi_pspl, loss_function='chi2')\n",
        "multi_fit.fit()\n",
        "\n",
        "print(\"Multi-telescope fit results:\")\n",
        "print(multi_fit.fit_results['best_model'])\n",
        "print(f\"\\nChi-squared: {multi_fit.fit_results['chi2']:.2f}\")\n",
        "\n",
        "# Plot results\n",
        "pyLIMA_plots.plot_lightcurves(multi_pspl, multi_fit.fit_results['best_model'])\n",
        "plt.title(\"Multi-Telescope pyLIMA Fit\")\n",
        "plt.show()\n",
        "\n",
        "# Show flux parameters for each telescope\n",
        "print(\"\\nFlux parameters for each telescope:\")\n",
        "param_names = list(multi_pspl.model_dictionnary.keys())\n",
        "for i, tel in enumerate(multi_event.telescopes):\n",
        "    flux_param = multi_fit.fit_results['best_model'][3 + i]  # Flux parameters start at index 3\n",
        "    print(f\"{tel.name} ({tel.camera_filter}): {flux_param:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pylima_performance"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 2.5 Performance and Best Practices </font>\n",
        "\n",
        "pyLIMA offers several fitting algorithms, each with different strengths:\n",
        "\n",
        "**Fitting Algorithms:**\n",
        "- **LM (Levenberg-Marquardt)**: Fast, good for initial fits\n",
        "- **DE (Differential Evolution)**: Robust, handles complex parameter spaces\n",
        "- **MCMC**: Best for uncertainty estimation and posterior sampling\n",
        "- **TRF (Trust Region Reflective)**: Good for constrained optimization\n",
        "\n",
        "**Best Practices:**\n",
        "- Start with LM for quick initial fits\n",
        "- Use DE for complex models or when LM fails\n",
        "- Use MCMC for final parameter estimation and uncertainties\n",
        "- Set appropriate parameter bounds for better convergence\n",
        "- Use custom parameters for better sampling (e.g., log_tE instead of tE)\n",
        "- Always check event setup with `check_event()`\n",
        "\n",
        "**Performance Tips:**\n",
        "- Use `rescale_photometry=True` in MCMC for better numerical stability\n",
        "- Set appropriate burn-in periods for MCMC chains\n",
        "- Use simulation capabilities to test your analysis pipeline\n",
        "- Consider using custom parameters for better sampling efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_section"
      },
      "source": [
        "## <font face=\"Helvetica\" size=\"6\"> 3. RTModel </font>\n",
        "\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "RTModel (Real-Time Model) is a sophisticated, hands-off microlensing modeling package developed by Valerio Bozza. Unlike other tools that require manual model selection, RTModel automatically determines the best model type for your event through a comprehensive grid search and template library approach. It's designed to be a \"set it up and press go\" solution that provides automated model interpretation.\n",
        "\n",
        "**Key Features:**\n",
        "- **Automated Model Selection**: Determines whether an event is single-lens, binary-lens, binary-source, etc.\n",
        "- **Template Library**: Uses pre-computed binary lens templates for efficient fitting\n",
        "- **Parallel Processing**: Exploits all available CPU cores for fast analysis\n",
        "- **Comprehensive Models**: Supports 7+ model categories including parallax and orbital motion\n",
        "- **Built-in Assessment**: Provides automatic interpretation of results\n",
        "- **Visualization Tools**: Includes plotting and animation capabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_installation"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.1 Installation and Setup </font>\n",
        "\n",
        "RTModel requires a C++17 compiler and uses VBMicrolensing for calculations. It can be installed via pip or from source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtmodel_install"
      },
      "outputs": [],
      "source": [
        "#@title Install RTModel\n",
        "# RTModel can be installed via pip\n",
        "!pip install RTModel\n",
        "\n",
        "# Import RTModel\n",
        "import RTModel\n",
        "import RTModel.plotmodel as plm\n",
        "\n",
        "print(\"RTModel installed and imported successfully!\")\n",
        "print(f\"RTModel version: {RTModel.__version__}\" if hasattr(RTModel, '__version__') else \"RTModel imported\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_data_prep"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.2 Data Preparation </font>\n",
        "\n",
        "RTModel requires a specific directory structure and data format. Each event needs its own directory with a `/Data` subdirectory containing the photometry files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtmodel_directory_setup"
      },
      "outputs": [],
      "source": [
        "#@title Create RTModel Directory Structure\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create event directory structure for RTModel\n",
        "event_dir = 'rtmodel_event001'\n",
        "data_dir = os.path.join(event_dir, 'Data')\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Created RTModel directory structure:\")\n",
        "print(f\"  {event_dir}/\")\n",
        "print(f\"  {event_dir}/Data/\")\n",
        "\n",
        "# RTModel requires this specific structure:\n",
        "# event_dir/\n",
        "# ├── Data/\n",
        "# │   ├── telescope1.dat\n",
        "# │   ├── telescope2.dat\n",
        "# │   └── event001.coordinates\n",
        "# └── (other files created by RTModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtmodel_data_format"
      },
      "outputs": [],
      "source": [
        "#@title Create RTModel Data Files\n",
        "# RTModel expects data in a specific format:\n",
        "# # Mag err HJD-2450000\n",
        "# 19.0232 0.012 8370.1223\n",
        "# 19.0150 0.011 8370.2421\n",
        "# ...\n",
        "\n",
        "# Let's create a sample data file using our synthetic microlensing data\n",
        "def create_rtmodel_data_file(filename, times, mags, mag_errs):\n",
        "    \"\"\"Create RTModel-compatible data file\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"# Mag err HJD-2450000\\n\")\n",
        "        for t, m, e in zip(times, mags, mag_errs):\n",
        "            # Convert to HJD-2450000 format\n",
        "            hjd_offset = t - 2450000\n",
        "            f.write(f\"{m:.4f} {e:.3f} {hjd_offset:.4f}\\n\")\n",
        "\n",
        "# Create sample data file\n",
        "data_file = os.path.join(data_dir, 'ogle.dat')\n",
        "create_rtmodel_data_file(data_file, times, mag, mag_err)\n",
        "\n",
        "print(f\"Created data file: {data_file}\")\n",
        "print(f\"Data points: {len(times)}\")\n",
        "\n",
        "# Show first few lines\n",
        "with open(data_file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    print(\"\\nFirst 5 lines of data file:\")\n",
        "    for i, line in enumerate(lines[:5]):\n",
        "        print(f\"  {i+1}: {line.strip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtmodel_coordinates"
      },
      "outputs": [],
      "source": [
        "#@title Create Coordinates File\n",
        "# RTModel requires event coordinates in a specific format\n",
        "# Format: HH:MM:SS.S +DD:PP:SS.S\n",
        "\n",
        "# Convert our coordinates to the required format\n",
        "def decimal_to_hms(ra_deg, dec_deg):\n",
        "    \"\"\"Convert decimal degrees to HH:MM:SS.S +DD:PP:SS.S format\"\"\"\n",
        "    # Convert RA (hours)\n",
        "    ra_hours = ra_deg / 15.0  # Convert degrees to hours\n",
        "    ra_h = int(ra_hours)\n",
        "    ra_m = int((ra_hours - ra_h) * 60)\n",
        "    ra_s = ((ra_hours - ra_h - ra_m/60) * 3600)\n",
        "    \n",
        "    # Convert Dec (degrees)\n",
        "    dec_sign = '+' if dec_deg >= 0 else '-'\n",
        "    dec_deg_abs = abs(dec_deg)\n",
        "    dec_d = int(dec_deg_abs)\n",
        "    dec_m = int((dec_deg_abs - dec_d) * 60)\n",
        "    dec_s = ((dec_deg_abs - dec_d - dec_m/60) * 3600)\n",
        "    \n",
        "    ra_str = f\"{ra_h:02d}:{ra_m:02d}:{ra_s:04.1f}\"\n",
        "    dec_str = f\"{dec_sign}{dec_d:02d}:{dec_m:02d}:{dec_s:04.1f}\"\n",
        "    \n",
        "    return f\"{ra_str} {dec_str}\"\n",
        "\n",
        "# Create coordinates file\n",
        "coords_file = os.path.join(data_dir, 'event001.coordinates')\n",
        "coords_str = decimal_to_hms(ra, dec)\n",
        "\n",
        "with open(coords_file, 'w') as f:\n",
        "    f.write(coords_str)\n",
        "\n",
        "print(f\"Created coordinates file: {coords_file}\")\n",
        "print(f\"Coordinates: {coords_str}\")\n",
        "print(f\"Original coordinates: RA={ra:.6f}°, Dec={dec:.6f}°\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_basic_usage"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.3 Basic Usage </font>\n",
        "\n",
        "RTModel is designed to be simple to use - just set up your data and run the analysis. The tool automatically determines the best model type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtmodel_initialization"
      },
      "outputs": [],
      "source": [
        "#@title Initialize RTModel\n",
        "# Create RTModel instance\n",
        "rtm = RTModel.RTModel(event_dir)\n",
        "\n",
        "# Check number of processors (RTModel uses all available by default)\n",
        "print(f\"RTModel initialized for event: {event_dir}\")\n",
        "print(f\"Available processors: {rtm.nprocessors}\")\n",
        "\n",
        "# You can limit the number of processors if needed\n",
        "# rtm.set_processors(4)  # Use only 4 processors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RTModel will automatically:\n",
        "  1. Pre-process the data\n",
        "  2. Generate initial conditions\n",
        "  3. Fit all model categories\n",
        "  4. Select best models\n",
        "  5. Provide final assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtmodel_run"
      },
      "outputs": [],
      "source": [
        "#@title Run RTModel Analysis\n",
        "# This is the main command - RTModel does everything automatically!\n",
        "print(\"Starting RTModel analysis...\")\n",
        "print(\"This may take several minutes depending on your machine.\")\n",
        "print(\"RTModel will fit 7 different model categories:\")\n",
        "print(\"  PS: Single-lens-single-source\")\n",
        "print(\"  PX: Single-lens-single-source with parallax\")\n",
        "print(\"  BS: Single-lens-binary-source\")\n",
        "print(\"  BO: Single-lens-binary-source with xallarap\")\n",
        "print(\"  LS: Binary-lens-single-source\")\n",
        "print(\"  LX: Binary-lens-single-source with parallax\")\n",
        "print(\"  LO: Binary-lens-single-source with orbital motion\")\n",
        "\n",
        "# Uncomment the line below to actually run RTModel\n",
        "# rtm.run()\n",
        "\n",
        "print(\"\\nNote: RTModel run commented out to avoid long execution.\")\n",
        "print(\"Uncomment rtm.run() to perform the actual analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_output"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.4 Understanding RTModel Output </font>\n",
        "\n",
        "After running RTModel, you'll get several output files and directories. Let's explore what each contains.\n",
        "\n",
        "**RTModel creates the following output structure:**\n",
        "\n",
        "```\n",
        "<event_dir>/\n",
        "├── Data/                    # Original input data\n",
        "├── ini/                     # Configuration files\n",
        "├── InitCond/                # Initial conditions for fitting\n",
        "├── Models/                  # Selected models for each category\n",
        "├── FinalModels/             # Best models from final assessment\n",
        "├── LCToFit.txt              # Pre-processed data\n",
        "├── FilterToData.txt         # Dataset mapping\n",
        "├── spline.txt               # Spline approximation points\n",
        "└── nature.txt               # Final assessment and best models\n",
        "```\n",
        "\n",
        "**Key output files:**\n",
        "  `nature.txt`: Contains the final assessment and list of best models\n",
        "  `FinalModels/`: Contains the best model files with parameters and uncertainties\n",
        "  `Models/`: Contains selected models for each category\n",
        "\n",
        "#### Understanding the nature.txt File\n",
        "\n",
        "The `nature.txt` file contains RTModel's assessment of the event\n",
        "\n",
        "**The nature.txt file contains:**\n",
        "\n",
        "1. Best chi-square for each model category:\n",
        "  - `PS`: Single-lens-single-source\n",
        "  - `PX`: Single-lens-single-source with parallax\n",
        "  - `BS`: Single-lens-binary-source\n",
        "  - `BO`: Single-lens-binary-source with xallarap\n",
        "  - `LS`: Binary-lens-single-source\n",
        "  - `LX`: Binary-lens-single-source with parallax\n",
        "  - `LO`: Binary-lens-single-source with orbital motion\n",
        "\n",
        "2. Final assessment of the event nature\n",
        "\n",
        "3. List of proposed best models\n",
        "\n",
        "4. Model comparison and interpretation\n",
        "\n",
        "**Example `nature.txt` content:**\n",
        "\n",
        "```nature.txt\n",
        "*********************\n",
        "****   RTModel   ****\n",
        "*********************\n",
        "\n",
        "Best chi-square for each category:\n",
        "PS: 1234.56 (Single-lens-single-source)\n",
        "PX: 1230.45 (Single-lens-single-source with parallax)\n",
        "BS: 1235.67 (Single-lens-binary-source)\n",
        "BO: 1232.34 (Single-lens-binary-source with xallarap)\n",
        "LS: 1238.90 (Binary-lens-single-source)\n",
        "LX: 1235.12 (Binary-lens-single-source with parallax)\n",
        "LO: 1236.78 (Binary-lens-single-source with orbital motion)\n",
        "\n",
        "Final Assessment:\n",
        "This appears to be a single-lens event with parallax effects.\n",
        "The best model is PX with chi-square = 1230.45\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_model_categories"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.5 Model Categories </font>\n",
        "\n",
        "RTModel fits 7 different model categories by default. Each has a specific label and parameter set.\n",
        "\n",
        "#### RTModel Model Categories\n",
        "\n",
        "| Label | Model | Parameters | Description |\n",
        "|-------|-------|------------|-------------|\n",
        "| `PS` | Single-lens-single-source | 4 | Basic microlensing (`t0`, `u0`, `tE`, `rho`) |\n",
        "| `PX` | Single-lens-single-source with parallax | 6 | Basic + parallax effects |\n",
        "| `BS` | Single-lens-binary-source | 7 | Two source stars, one lens |\n",
        "| `BO` | Single-lens-binary-source with xallarap | 10 | Binary source + orbital motion |\n",
        "| `LS` | Binary-lens-single-source | 7 | Two lens components (planetary) |\n",
        "| `LX` | Binary-lens-single-source with parallax | 9 | Binary lens + parallax |\n",
        "| `LO` | Binary-lens-single-source with orbital motion | 12 | Binary lens + orbital motion |\n",
        "\n",
        "#### Additional categories available:\n",
        "- `LK`: Binary-lens-single-source with eccentric orbital motion (14 parameters)\n",
        "\n",
        "**Parameter details for each model category:**\n",
        "\n",
        "| Parameter | PS | PX | BS | BO | LS | LX | LO | Notes |\n",
        "|-----------|----|----|----|----|----|----|----|-------|\n",
        "| `u0` | ✅* | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | Impact parameter |\n",
        "| `tE` | ✅* | ✅* | ✅* | ✅* | ✅* | ✅* | ✅* | Einstein time in days |\n",
        "| `t0` | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | Closest approach time in HJD |\n",
        "| `rho` | ✅* | ✅* | ✅* | ✅* | ✅* | ✅* | ✅* | Source radius |\n",
        "| `piN` | ❌ | ✅ | ❌ | ❌ | ❌ | ✅ | ❌ | Parallax component along North |\n",
        "| `piE` | ❌ | ✅ | ❌ | ❌ | ❌ | ✅ | ❌ | Parallax component along East |\n",
        "| `s` | ❌ | ❌ | ❌ | ❌ | ✅* | ✅* | ✅* | Separation between lenses |\n",
        "| `q` | ❌ | ❌ | ❌ | ❌ | ✅* | ✅* | ✅* | Mass ratio |\n",
        "| `alpha` | ❌ | ❌ | ❌ | ❌ | ✅ | ✅ | ✅ | Angle of source trajectory |\n",
        "| `q_S` | ❌ | ❌ | ✅ | ✅ | ❌ | ❌ | ❌ | Source flux ratio |\n",
        "| `rho_S` | ❌ | ❌ | ✅* | ✅* | ❌ | ❌ | ❌ | Source separation |\n",
        "| `theta_S` | ❌ | ❌ | ✅ | ✅ | ❌ | ❌ | ❌ | Source angle |\n",
        "\n",
        "> key: `*` - ln scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_visualization"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.6 Visualization and Results </font>\n",
        "\n",
        "RTModel includes built-in visualization tools through the `plotmodel` subpackage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtmodel_plotting"
      },
      "outputs": [],
      "source": [
        "#@title RTModel Plotting\n",
        "# RTModel includes plotting capabilities\n",
        "\n",
        "# 1. Light curve plotting:\n",
        "plm.plotmodel(eventname, modelfile) # Plot light curve with model\n",
        "plm.plotmodel(eventname, modelfile, show_caustics=True) # Show caustics\n",
        "\n",
        "# 2. Animation capabilities:\n",
        "plm.animate_model(eventname, modelfile) # Create animated GIF\n",
        "\n",
        "# 3. Astrometric plots:\n",
        "plm.plot_astrometry(eventname, modelfile) # Plot astrometric trajectory\n",
        "\n",
        "# Example usage:\n",
        "# After running RTModel and getting results\n",
        "plm.plotmodel(eventname='rtmodel_event001', modelfile='FinalModels/PS0001.txt')\n",
        "plm.plotmodel(eventname='rtmodel_event001', modelfile='FinalModels/LS0001.txt', show_caustics=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### How to analyze RTModel results:\n",
        "\n",
        "1. Check nature.txt for the final assessment\n",
        "\n",
        "   - Best chi-square for each model category\n",
        "   - RTModel's interpretation of the event\n",
        "   - List of proposed best models\n",
        "\n",
        "2. Examine FinalModels/ directory\n",
        "\n",
        "   - Each file contains parameters and uncertainties\n",
        "   - First line: parameters + fluxes + chi-square\n",
        "   - Second line: parameter uncertainties\n",
        "   - Remaining lines: covariance matrix\n",
        "\n",
        "3. Compare models using chi-square\n",
        "\n",
        "   - Lower chi-square = better fit\n",
        "   - Consider degrees of freedom (more parameters = higher expected chi-square)\n",
        "   - Use F-test or AIC/BIC for model comparison\n",
        "\n",
        "4. Validate results\n",
        "\n",
        "   - Check parameter uncertainties\n",
        "   - Examine residuals\n",
        "   - Look for systematic effects\n",
        "   - Consider physical plausibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_advanced"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.7 Advanced Features </font>\n",
        "\n",
        "RTModel offers several advanced features for customization and control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title: 1. Model Category Selection:\n",
        "\n",
        "# Fit only specific model categories\n",
        "rtm.set_model_categories(['PS', 'LS'])  # Only single-lens and binary-lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title: 2. Fitting Options:\n",
        "\n",
        "# Configure Levenberg-Marquardt fitting\n",
        "rtm.config_LevMar(nfits=10, timelimit=1200.0, maxsteps=100)\n",
        "#@title: 3. Parameter Constraints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title: 3. Parameter Constraints:\n",
        "\n",
        "# Set gaussian constraints on parameters\n",
        "rtm.set_constraints('tE', 25.0, 2.0)  # tE = 25 ± 2 days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title: 4. Parallel Processing:\n",
        "\n",
        "# Control number of processors\n",
        "rtm.set_processors(8)  # Use 8 processors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title: 5. Step-by-step Execution:\n",
        "\n",
        "# Run individual steps manually\n",
        "rtm.Reader()           # Data pre-processing\n",
        "rtm.InitCond()        # Generate initial conditions\n",
        "rtm.launch_fits('PS') # Fit specific category\n",
        "rtm.ModelSelection()  # Select best models\n",
        "rtm.FinalAssessment() # Final assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_best_practices"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.8 RTModel Best Practices </font>\n",
        "\n",
        "**1. Data Quality:**\n",
        "- Ensure data is properly formatted\n",
        "- Check for systematic errors\n",
        "- Verify coordinate accuracy\n",
        "\n",
        "**2. Computational Resources:**\n",
        "- Use all available processors for speed\n",
        "- Allow sufficient time (1-3 hours typical)\n",
        "- Ensure adequate disk space\n",
        "\n",
        "**3. Model Interpretation:**\n",
        "- Don't rely solely on chi-square\n",
        "- Consider physical plausibility\n",
        "- Check parameter uncertainties\n",
        "- Validate against known physics\n",
        "\n",
        "**4. Troubleshooting:**\n",
        "- Check ini/ directory for configuration\n",
        "- Examine error messages in output\n",
        "- Verify data format compliance\n",
        "- Use step-by-step execution for debugging\n",
        "\n",
        "**5. Results Validation:**\n",
        "- Compare with other fitting codes\n",
        "- Check for systematic residuals\n",
        "- Verify parameter correlations\n",
        "- Consider alternative models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_comparison"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.9 RTModel vs Other Tools </font>\n",
        "\n",
        "RTModel offers a unique approach compared to other microlensing fitting tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_comparison_table"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> Comparison with Other Tools </font>\n",
        "\n",
        "**RTModel vs Other Microlensing Tools:**\n",
        "\n",
        "| Feature | RTModel | MulensModel | pyLIMA |\n",
        "|---------|---------|-------------|--------|\n",
        "| Model Selection | Automatic | Manual | Manual |\n",
        "| Ease of Use | Very Easy | Moderate | Moderate |\n",
        "| Speed | Fast (parallel) | Variable | Variable |\n",
        "| Automation | High | Low | Low |\n",
        "| Customization | Moderate | High | High |\n",
        "| Visualization | Built-in | Basic | Good |\n",
        "| Parallel Processing | Yes | Manual | Manual |\n",
        "\n",
        "**RTModel Advantages:**\n",
        "- • Hands-off operation - just set up data and run\n",
        "- • Automatic model selection and interpretation\n",
        "- • Template library for efficient binary lens fitting\n",
        "- • Built-in assessment and visualization\n",
        "- • Parallel processing for speed\n",
        "\n",
        "**RTModel Limitations:**\n",
        "- • Less customization than manual tools\n",
        "- • Requires specific data format\n",
        "- • Limited to supported model categories\n",
        "- • May miss subtle effects in complex events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_astrometric"
      },
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 3.10 Astrophotometric Fitting (RTModel v3.0) </font>\n",
        "\n",
        "**NEW IN RTModel v3.0!** RTModel now supports combined photometric and astrometric fitting, making it the first tool to offer comprehensive astrophotometric microlensing analysis. This capability is crucial for space-based observations like Roman, where milliarcsecond astrometric precision enables measurement of the centroid trajectory during microlensing events.\n",
        "\n",
        "**Key Astrometric Features:**\n",
        "- **Combined Fitting**: Simultaneous photometric and astrometric parameter estimation\n",
        "- **Centroid Trajectory**: Models the astrometric shift of images during events\n",
        "- **Proper Motion**: Measures source and lens proper motions\n",
        "- **Einstein Angle**: Direct measurement of θ_E from astrometry\n",
        "- **Source Parallax**: Geometric parallax of the source star\n",
        "- **Visualization**: Built-in astrometric plotting and trajectory visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_astrometric_data"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 3.10.1 Astrophotometric Data Format </font>\n",
        "\n",
        "Astrophotometric datasets include both photometric and astrometric information in a single file.\n",
        "\n",
        "Astrophotometric datasets have 7 columns: `Mag`, `err`, `HJD-2450000`, `Dec`, `errDec`, `RA`, `errRA`\n",
        "\n",
        "**Example astrophotometric data:**\n",
        "\n",
        "```\n",
        "# Mag err HJD-2450000 Dec errDec RA errRA\n",
        "19.0232 0.012 8370.1223 1.2534 1.0 0.0165 1.0\n",
        "19.0150 0.011 8370.2421 1.7510 1.1 -0.5422 1.1\n",
        "19.0034 0.011 8370.3697 1.1190 1.1 -0.1981 1.1\n",
        "18.9712 0.010 8370.4911 1.4281 1.0 0.2119 1.0\n",
        "18.9592 0.011 8370.6114 1.3005 0.9 0.3982 1.0\n",
        "18.9109 0.009 8370.8234 1.6233 1.0 0.5121 1.0\n",
        "18.8798 0.009 8371.0092 2.0223 1.2 0.9411 1.1\n",
        "```\n",
        "\n",
        "**Column descriptions:**\n",
        "\n",
        "  - `Mag`: Magnitude\n",
        "  - `err`: Photometric error\n",
        "  - `HJD-2450000`: Heliocentric Julian Date - 2450000\n",
        "  - `Dec`: Declination offset in milliarcseconds\n",
        "  - `errDec`: Declination error in milliarcseconds\n",
        "  - `RA`: Right Ascension offset in milliarcseconds\n",
        "  - `errRA`: Right Ascension error in milliarcseconds\n",
        "\n",
        "> Note: Dec and RA are angular displacements from a fixed reference point\n",
        "in the North and East directions respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtmodel_astrometric_create"
      },
      "outputs": [],
      "source": [
        "#@title Create Astrophotometric Data\n",
        "# Create sample astrophotometric data\n",
        "def create_astrophotometric_data(filename, times, mags, mag_errs):\n",
        "    \"\"\"Create RTModel-compatible astrophotometric data file\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"# Mag err HJD-2450000 Dec errDec RA errRA\\n\")\n",
        "        for t, m, e in zip(times, mags, mag_errs):\n",
        "            # Convert to HJD-2450000 format\n",
        "            hjd_offset = t - 2450000\n",
        "            \n",
        "            # Generate synthetic astrometric data\n",
        "            # Simulate centroid motion during microlensing event\n",
        "            t0 = 2459123.5  # Peak time\n",
        "            tE = 25.0       # Einstein time\n",
        "            \n",
        "            # Simple astrometric model (centroid shift)\n",
        "            u = abs(t - t0) / tE\n",
        "            if u < 0.1:  # Near peak\n",
        "                dec_offset = 2.0 * np.exp(-u**2)  # Peak shift\n",
        "                ra_offset = 1.5 * np.exp(-u**2)   # Peak shift\n",
        "            else:\n",
        "                dec_offset = 0.5 * np.exp(-u**2)  # Background\n",
        "                ra_offset = 0.3 * np.exp(-u**2)   # Background\n",
        "            \n",
        "            # Add noise\n",
        "            dec_offset += np.random.normal(0, 0.1)\n",
        "            ra_offset += np.random.normal(0, 0.1)\n",
        "            \n",
        "            # Astrometric errors (typically 0.5-1.0 mas)\n",
        "            dec_err = 0.8 + 0.2 * np.random.random()\n",
        "            ra_err = 0.8 + 0.2 * np.random.random()\n",
        "            \n",
        "            f.write(f\"{m:.4f} {e:.3f} {hjd_offset:.4f} {dec_offset:.4f} {dec_err:.1f} {ra_offset:.4f} {ra_err:.1f}\\n\")\n",
        "\n",
        "# Create astrophotometric data file\n",
        "astro_data_file = os.path.join(data_dir, 'roman_astrometric.dat')\n",
        "create_astrophotometric_data(astro_data_file, times, mag, mag_err)\n",
        "\n",
        "print(f\"Created astrophotometric data file: {astro_data_file}\")\n",
        "print(f\"Data points: {len(times)}\")\n",
        "\n",
        "# Show first few lines\n",
        "with open(astro_data_file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    print(\"\\nFirst 5 lines of astrophotometric data:\")\n",
        "    for i, line in enumerate(lines[:5]):\n",
        "        print(f\"  {i+1}: {line.strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_astrometric_parameters"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 3.10.2 Astrophotometric Parameters </font>\n",
        "\n",
        "When RTModel detects astrophotometric data, it automatically adds 4 additional parameters to the fit:\n",
        "\n",
        "| Parameter | Meaning | Physical Significance |\n",
        "|-----------|---------|---------------------|\n",
        "| `muS_Dec` | Source proper motion component in North direction (mas/yr) | Source motion perpendicular to line of sight |\n",
        "| `muS_RA` | Source proper motion component in East direction (mas/yr) | Source motion perpendicular to line of sight |\n",
        "| `piS` | Geometric parallax of the source (mas) | Distance to source star |\n",
        "| `thetaE` | Einstein angle (mas) | Lens mass and distance (θ_E = √(4GM/c²D_L)) |\n",
        "\n",
        "**Model Categories for Astrophotometric Fits:**\n",
        "* `PX`: Single-lens-single-source with parallax (10 parameters)\n",
        "* `BO`: Single-lens-binary-source with xallarap (14 parameters)\n",
        "* `LX`: Binary-lens-single-source with parallax (13 parameters)\n",
        "* `LO`: Binary-lens-single-source with orbital motion (16 parameters)\n",
        "* `LK`: Binary-lens-single-source with eccentric orbital motion (18 parameters)\n",
        "\n",
        "> Note: No static models are fitted in astrophotometric mode because parallax is always included to model the centroid trajectory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_astrometric_fitting"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 3.10.3 Astrophotometric Fitting </font>\n",
        "\n",
        "RTModel automatically detects astrophotometric datasets and switches to combined fitting mode.\n",
        "\n",
        "**RTModel Astrophotometric Fitting:**\n",
        "\n",
        "1. Automatic Detection:\n",
        "\n",
        "  RTModel automatically detects astrophotometric datasets by checking for 7-column format (vs 3-column for photometric)\n",
        "\n",
        "2. Combined Chi-Square:\n",
        "\n",
        "  * $\\chi^2_{total} = \\chi^2_{photometric} + \\chi^2_{astrometric}$\n",
        "  * Both contributions are weighted appropriately\n",
        "\n",
        "3. Enhanced Parameter Space:\n",
        "\n",
        "   - 4 additional astrometric parameters\n",
        "   - Parallax always included (no static models)\n",
        "   - More complex model selection\n",
        "\n",
        "4. Example Usage:\n",
        "\n",
        "  ```python\n",
        "  # RTModel automatically detects astrophotometric data\n",
        "  rtm = RTModel.RTModel('astro_event')\n",
        "  rtm.run()  # Automatically uses astrophotometric fitting\n",
        "  ```\n",
        "\n",
        "5. Output Structure:\n",
        "\n",
        "  * Model files contain: `nps + 4*ntel + 1` parameters\n",
        "   - `nps`: model parameters + 4 astrometric parameters\n",
        "   - `4*ntel`: fluxes and centroid positions for each telescope\n",
        "   - `+1`: total chi-square"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_astrometric_visualization"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 3.10.4 Astrometric Visualization </font>\n",
        "\n",
        "RTModel includes specialized plotting functions for astrometric data visualization.\n",
        "\n",
        "**RTModel Astrometric Plotting Functions:**\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title: 1. Combined Light Curve and Astrometry\n",
        "myplot = plm.plotmodel(eventname, modelfile)\n",
        "# Shows both photometric light curve and astrometric parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title: 2. Centroid Trajectory in the Sky:\")\n",
        "myplot.showastrometry()\n",
        "# Shows the centroid trajectory as a function of time\n",
        "# Displays RA vs Dec with error ellipses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title: 3. Individual Astrometric Components:\")\n",
        "myplot.showastrometryRA()  # RA vs time\n",
        "myplot.showastrometryDec() # Dec vs time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title: 4. Multiple Telescope Support:\")\n",
        "myplot.showastrometry(1)  # Show astrometry for telescope 1\n",
        "# Different telescopes may have different blending fractions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Key Features:\n",
        "\n",
        "   - Automatic error ellipse plotting\n",
        "   - Time evolution of centroid position\n",
        "   - Comparison with observed astrometric data\n",
        "   - Proper motion vector visualization\n",
        "\n",
        "6. Example Output:\n",
        "   - Centroid trajectory shows the astrometric microlensing signal\n",
        "   - Peak season data has highest precision\n",
        "   - Post-peak data constrains proper motion and parallax\n",
        "   - Error bars reflect astrometric precision (typically 0.5-1.0 mas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Physical Interpretation of Astrophotometric Parameters:**\n",
        "\n",
        "1. Einstein Angle ($\\theta_E$)\n",
        "> $\\theta_E = \\sqrt{(4GM/c^2D_L)}$\n",
        "- Direct measurement of lens mass and distance\n",
        "- Breaks the mass-distance degeneracy\n",
        "- Typical values: 0.1-1.0 mas for stellar lenses\n",
        "\n",
        "2. Source Proper Motion ($\\mu_S$)\n",
        "> $\\mu_S = v_S / D_S$\n",
        "- Tangential velocity of source star\n",
        "- Helps constrain source distance and kinematics\n",
        "- Typical values: 1-10 mas/yr for Galactic sources\n",
        "\n",
        "3. Source Parallax ($\\pi_S$)\n",
        "$\\pi_S = 1/D_S$\n",
        "- Geometric parallax of source star\n",
        "- Provides independent distance measurement\n",
        "- Typical values: $0.1-1.0 \\,\\text{mas}$ for Galactic sources\n",
        "\n",
        "4. Centroid Trajectory\n",
        "- Shows the motion of the light centroid during the event\n",
        "- Amplitude depends on lens mass and source-lens separation\n",
        "- Shape reveals lens geometry (single vs binary)\n",
        "- Duration related to Einstein time\n",
        "\n",
        "5. Advantages of Astrophotometric Fitting\n",
        "- Breaks degeneracies in lens mass and distance\n",
        "- Provides independent constraints on source properties\n",
        "- Enables direct measurement of Einstein angle\n",
        "- Improves parameter precision and accuracy\n",
        "- Essential for space-based microlensing surveys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtmodel_astrometric_applications"
      },
      "source": [
        "#### <font face=\"Helvetica\" size=\"4\"> 3.10.5 Applications and Use Cases </font>\n",
        "\n",
        "Astrophotometric fitting is particularly valuable for specific types of observations and scientific goals.\n",
        "\n",
        "**Key Applications of Astrophotometric Microlensing:**\n",
        "\n",
        "1. Space-Based Surveys (Roman, Euclid)\n",
        "- Milliarcsecond astrometric precision\n",
        "- Wide-field astrometric capabilities\n",
        "- Long-term monitoring of events\n",
        "- Essential for mass and distance measurements\n",
        "\n",
        "2. Adaptive Optics Observations\n",
        "- Ground-based high-resolution imaging\n",
        "- Resolved source and lens components\n",
        "- Precise centroid measurements\n",
        "- Complementary to photometric data\n",
        "\n",
        "3. Planetary Microlensing\n",
        "- Direct measurement of planet masses\n",
        "- Breaking mass-distance degeneracies\n",
        "- Improved orbital parameter constraints\n",
        "- Better characterization of planetary systems\n",
        "\n",
        "4. Free-Floating Planets\n",
        "- Mass measurement without host star\n",
        "- Distance determination\n",
        "- Population statistics\n",
        "- Formation mechanism constraints\n",
        "\n",
        "5. Binary Lens Systems\n",
        "- Improved caustic crossing predictions\n",
        "- Better orbital motion constraints\n",
        "- Enhanced binary parameter determination\n",
        "- More accurate mass ratio measurements\n",
        "\n",
        "6. Galactic Structure Studies\n",
        "- Source and lens distance measurements\n",
        "- Proper motion distributions\n",
        "- Kinematic information\n",
        "- Population synthesis constraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Best Practices for Astrophotometric Fitting:**\n",
        "\n",
        "1. Data Quality Requirements\n",
        "- Astrometric precision: $< 1 \\,\\text{mas}$ (preferably $< 0.5 \\,\\text{mas}$)\n",
        "- Temporal coverage: Peak + post-peak monitoring\n",
        "- Reference frame stability\n",
        "- Proper error estimation\n",
        "\n",
        "2. Observational Strategy\n",
        "- High-cadence observations during peak\n",
        "- Long-term follow-up for proper motion\n",
        "- Multiple epochs for parallax measurement\n",
        "- Consistent reference frame\n",
        "\n",
        "3. Model Selection\n",
        "- Start with simple models (`PX`)\n",
        "- Add complexity as needed (`LX`, `LO`)\n",
        "- Consider eccentric orbital motion (`LK`)\n",
        "- Validate against physical constraints\n",
        "\n",
        "### 4. Parameter Validation\n",
        "- Check Einstein angle physical plausibility\n",
        "- Verify proper motion consistency\n",
        "- Validate parallax measurements\n",
        "- Compare with independent constraints\n",
        "\n",
        "### 5. Interpretation Challenges\n",
        "- Blending effects on centroid\n",
        "- Reference frame systematics\n",
        "- Proper motion vs parallax degeneracy\n",
        "- Binary lens complexity\n",
        "\n",
        "### 6. Future Prospects\n",
        "- Roman Space Telescope: $\\sim 0.1 \\,\\text{mas}$ precision\n",
        "- Euclid: Wide-field astrometry\n",
        "- Ground-based AO: High-resolution follow-up\n",
        "- Multi-wavelength astrometry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <font face=\"Helvetica\" size=\"6\"> 4. popclass: Probabilistic Classification of Microlensing Lenses </font>\n",
        "\n",
        "<hr style=\"border: 1.5pt solid #a859e4; width: 100%; margin-top: -10px;\">\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> What is popclass? </font>\n",
        "\n",
        "**popclass** is a Python package developed by LLNL (lead: Peter McGill) for *probabilistic classification* of the lens in a gravitational microlensing event. It bridges the gap between event posteriors (from light curve modeling) and population synthesis simulations of the Milky Way, allowing you to infer the probability that a given event was caused by a star, white dwarf, neutron star, or black hole.\n",
        "\n",
        "- **Key use:** Given posterior samples (e.g., from MulensModel, pyLIMA, or any Bayesian fit) and a Galactic population model, popclass computes the probability that the lens belongs to each class.\n",
        "- **Why use it?** It enables population-level inference, e.g., identifying likely black hole events, and is designed for the era of large surveys (Roman, Rubin)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 4.1 Installation </font>\n",
        "\n",
        "```bash\n",
        "pip install popclass\n",
        "```\n",
        "or via conda:\n",
        "```bash\n",
        "conda install -c conda-forge popclass\n",
        "```\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> 4.2 Basic Usage Example </font>\n",
        "\n",
        "Suppose you have posterior samples for an event in log10(tE) and log10(piE):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from popclass.posterior import Posterior\n",
        "from popclass.model import PopulationModel\n",
        "from popclass.classify import classify\n",
        "\n",
        "# Mock posterior samples\n",
        "NUM_POSTERIOR_SAMPLES = 10000\n",
        "logtE = np.random.normal(loc=2, scale=0.1, size=NUM_POSTERIOR_SAMPLES)\n",
        "logpiE = np.random.normal(loc=-1, scale=0.5, size=NUM_POSTERIOR_SAMPLES)\n",
        "posterior_samples = np.vstack((logtE, logpiE)).T\n",
        "prior_density = 0.028 * np.ones(NUM_POSTERIOR_SAMPLES)  # uniform prior\n",
        "\n",
        "# Wrap in popclass objects\n",
        "posterior = Posterior(samples=posterior_samples, parameter_labels=['log10tE', 'log10piE'])\n",
        "inference_data = posterior.to_inference_data(prior_density)\n",
        "\n",
        "# Load a pre-built population model (e.g., PopSyCLE)\n",
        "popsycle = PopulationModel.from_library('popsycle_singles_sukhboldn20')\n",
        "\n",
        "# Classify!\n",
        "classification = classify(population_model=popsycle, inference_data=inference_data, parameters=['log10tE', 'log10piE'])\n",
        "print(classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output:**\n",
        "`{'black_hole': 0.09, 'neutron_star': 0.001, 'star': 0.71, 'white_dwarf': 0.20}`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font face=\"Helvetica\" size=\"5\"> 4.3 How It Works </font>\n",
        "\n",
        "- **Inputs:** Posterior samples for event parameters (e.g., `tE`, `piE`), their prior densities, and a Galactic population model (e.g., from PopSyCLE).\n",
        "- **Method:** Uses Bayesian importance sampling to compare the event's posterior to simulated populations, computing the probability for each lens class.\n",
        "- **Output:** A dictionary of class probabilities (star, white dwarf, neutron star, black hole).\n",
        "\n",
        "**Mathematical summary:**\n",
        "\\[\n",
        "p(\\text{class}_L| \\boldsymbol{d}, \\mathcal{G}) = \\frac{p(\\text{class}_L| \\mathcal{G})}{p(\\boldsymbol{d}| \\mathcal{G})}\n",
        "    \\times \\frac{1}{S} \\sum _{c=0}^{S} \\frac{p(\\theta _c | \\text{class}_L, \\mathcal{G})}{\\pi(\\theta _{c})}\n",
        "\\]\n",
        "where:\n",
        "- \\( \\boldsymbol{d} \\): event data\n",
        "- \\( \\mathcal{G} \\): Galactic model\n",
        "- \\( \\theta_c \\): posterior samples\n",
        "- \\( \\pi(\\theta_c) \\): prior density\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> 4.4 Population Models </font>\n",
        "\n",
        "popclass comes with several pre-built population models (from PopSyCLE), e.g.:\n",
        "- `popsycle_singles_sukhboldn20`\n",
        "- `popsycle_singles_raithel18`\n",
        "- `popsycle_singles_spera15`\n",
        "\n",
        "You can also load your own models in ASDF format.\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> 4.5 Advanced Features </font>\n",
        "\n",
        "- **Flexible parameter spaces:** Use any set of event parameters (e.g., `tE`, `piE`, `thetaE`, `blend_fraction`).\n",
        "- **Custom population models:** Supply your own simulation data in the required format.\n",
        "- **Uncertainty quantification:** Includes a \"None\" class to flag events not well explained by the model.\n",
        "- **ArviZ and PyMultiNest integration:** For handling posteriors from common Bayesian tools.\n",
        "- **Plotting:** Built-in tools for visualizing classification results.\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> 4.6 Best Practices & Caveats </font>\n",
        "\n",
        "- **Parameter consistency:** Ensure your posterior and population model use the same parameterization (e.g., both in `log10(tE)`, not `tE`).\n",
        "- **Prior density:** Must match the parameter space of the population model (apply Jacobian if transforming variables).\n",
        "- **Model completeness:** If your event is outside the simulated parameter space, the \"None\" class will help flag this.\n",
        "- **Interpretation:** Probabilities are only as good as the population model and the event posterior.\n",
        "\n",
        "### <font face=\"Helvetica\" size=\"5\"> 4.7 Further Reading </font>\n",
        "\n",
        "- [popclass documentation](https://popclass.readthedocs.io)\n",
        "- [PopSyCLE population models](https://github.com/jluastro/PopSyCLE)\n",
        "- Perkins et al. (2024), Kaczmarek et al. (2024) — theoretical background\n",
        "\n",
        "> **popclass** is a powerful tool for population-level inference in microlensing, enabling robust, probabilistic classification of lens types for large event samples."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "B5OeZIbHv2eB"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
